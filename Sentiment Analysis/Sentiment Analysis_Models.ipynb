{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach for Sentiment Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninosyonan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total running time:  3.899999999923409e-05\n"
     ]
    }
   ],
   "source": [
    "# first we import all the necessary packages for the tutorial\n",
    "    # for more information please check the helperfunction.py file\n",
    "\n",
    "from helperfunctions import *\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we initialize the random number generator to a constant value so that we can easily reproduce results\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000, 500)\n",
      "Documents adjusted: all reviews have the same length of 500 words.\n"
     ]
    }
   ],
   "source": [
    "# load the imdb data set and sequence the dataset to a maximum review length of 500 words\n",
    "    # pad each document to ensure that they are of the same length\n",
    "    # we used the keras utility to pad the dataset to a length of 500 for each observation\n",
    "    # longer sequences are truncated and shorter sequences are padded with zeros at the end\n",
    "# we will focus onlyon the first 5000 most used words in the dataset\n",
    "    # because the dataset has a built in fixed dictionary of the 5000 most frequent tokens\n",
    "\n",
    "top_words = 5000\n",
    "max_words = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(\"Documents adjusted: all reviews have the same length of\",max_words,\"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x1a3427c2b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keras Embedding Layer\n",
    "    # it requires the input data to be integer encoded, so that each word is represented by a unique integer\n",
    "    # Embedding layer is initialized with random weights and will learn an embedding for all words in the training dataset\n",
    "\n",
    "# below we define an Embedding layer with a vocabulary of 5000\n",
    "    # integer encoded words from 0 to 4999\n",
    "    # a vector space of 32 dimensions in which words will be embedded\n",
    "    # and input documents that have 500 words each\n",
    "Embedding(5000, 32, input_length=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build MLP model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build MLP model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#we used binary cross entropy loss here because it is a binary classification problem\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 31s - loss: 0.5124 - acc: 0.7084 - val_loss: 0.3435 - val_acc: 0.8492\n",
      "Epoch 2/2\n",
      " - 35s - loss: 0.1929 - acc: 0.9260 - val_loss: 0.3006 - val_acc: 0.8740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30c21b70>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.40%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Convolutional Neural Networks\n",
    "* How do we go beyond words (sentences and paragraphs)?<br><br>\n",
    "* This turns to be a very hard problem?<br><br>\n",
    "* Simple Approaches:\n",
    "   * Word Vector Averaging\n",
    "   * Weighted Word Vector Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "#### Features\n",
    "* Excellent feature Extractors<br>\n",
    "* Features are detected regardless of position in image<br>\n",
    "* NLP for Text: Collobert et all 2011<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/CNN_arc.png\" alt=\"CNN_arc\" style=\"width: 900px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the Network:\n",
    "* Convolutional Layer: <br>\n",
    "    * In image analysis, our filters slide over local patches of an image, but in NLP we typically use filters that <br>\n",
    "    slide over full rows of the matrix (words)<br>\n",
    "    * CNN learns the values of these filters on its own during the training process!<br><br>\n",
    "* Max Pooling: <br>\n",
    "    * reduces the spatial size of the input representation<br>\n",
    "    * pooling makes the input representations (feature dimension) smaller and more manageable<br>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN for NLP Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Step:\n",
    "<img src=\"images/CNN1.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Step:\n",
    "<img src=\"images/CNN3.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Step:\n",
    "<img src=\"images/CNN2.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Step:\n",
    "<img src=\"images/CNN4.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Step:\n",
    "<img src=\"images/CNN5.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Step:\n",
    "<img src=\"images/CNNa.jpeg\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Step:\n",
    "<img src=\"images/CNNb.jpeg\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Step:\n",
    "<img src=\"images/CNN6.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Step:\n",
    "<img src=\"images/CNN7.png\" alt=\"CNN_arc\" style=\"width: 600px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build CNN model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "# Embedding: turns positive integers (indexes) into dense vectors of fixed size\n",
    "# Conv:\n",
    "    # filter: dimensionality of the output space (32 dimensions) \n",
    "    # kernel_size: reads (window) embedded word representations 3 vector elements of the word embedding at a time\n",
    "    # padding: \"same\" results in padding the input such that the output has the same length as the original input\n",
    "    # activation: activation function to use is 'relu': relu has better properties and speeds up the training\n",
    "# MaxPooling:\n",
    "    # pool_size: 2 the pooling layer is used to reduce the amount of parameters to simplify the computation\n",
    "# Flatten\n",
    "    # to connect a Dense layer directly to an Embedding layer, flatten the 2D output matrix to a 1D vector\n",
    "# Dense (sigmoid): sigmoid activation will produce a float number between 0 and 1\n",
    "\n",
    "print('Build CNN model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, \n",
    "                 kernel_size=3, \n",
    "                 padding='same', \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# we used binary cross entropy loss here because it is a binary classification problem\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 37s - loss: 0.5125 - acc: 0.7139 - val_loss: 0.2862 - val_acc: 0.8810\n",
      "Epoch 2/2\n",
      " - 36s - loss: 0.2244 - acc: 0.9127 - val_loss: 0.2707 - val_acc: 0.8877\n"
     ]
    }
   ],
   "source": [
    "# epochs: it passed 2 times through the full training set\n",
    "# batch size: the number of training examples in one forward/backward pass\n",
    "# vebose: 2 = one line per epoch\n",
    "    # running the example (accuracy of 88.73%) offers a small improvement over the neural network model above\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.77%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "* Running the example offers a improvement over the MLP above with an accuracy of nearly 88.64%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 10s 385us/step\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)\n",
    "preds = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "* Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-88f7152c3474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# plot the confusion Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'positive'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# we want to setup and generate a cofusion matrix to have get a better understanding of the evaluation\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# plot the confusion Matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plot_confusion_matrix(cm, {'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "   * Total of 2841 wrong classifications and 22159 true classifications<br><br>\n",
    "   * Classifier predicted \"negative\" 12503 times, and \"positive\" 12497 times for the reviews<br><br>\n",
    "   * True negative: 11081 | False positive: 1419<br><br>\n",
    "   * False negative: 1422 | True positive: 11078<br><br>\n",
    "   * Accuracy: 88,64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### In this tutorial, we discovered the topic of Sentiment Analysis with the Keras IMDB dataset.<br><br>\n",
    "\n",
    "* #### We learned how to develop deep learning models for sentiment analysis including:\n",
    "    * How to handle the basic dictionary approach for sentiment analysis<br><br>\n",
    "    * How to load review and analyze the IMDB dataset within Keras<br><br>\n",
    "    * How to use and build word embeddings with the Keras Embedding Layer for deep learning<br><br>\n",
    "    * How to develop a one-dimensional CNN model for sentiment analysis and how it works for NLP<br><br>\n",
    "    \n",
    "* #### How to continue with this tutorial?\n",
    "    * Try to experiment with the number of features such as filter size in the convolutional layer<br><br>\n",
    "    * You can also experiment with several convolutional layers and maxpooling layers, etc.<br><br>\n",
    "    * Try to obtain higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Limitations and further Topics\n",
    "\n",
    "* CNNs are not able to encode long-range dependencies, and therefore, for some language modeling tasks, where long-distance dependence matters, other  architectures are preferred:<br><br>\n",
    "    * Recurrent Neural Networks (RNN)<br><br>\n",
    "    * Long Short Term memory (LSTM) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
