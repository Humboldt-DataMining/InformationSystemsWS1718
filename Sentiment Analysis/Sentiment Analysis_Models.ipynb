{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Approach for Sentiment Classification \n",
    "\n",
    "In the following code section you will find our machine learning approach for the sentiment classification task on the Keras IMDB dataset. \n",
    "\n",
    "First, we will load the dataset as done before. The input_dim describes the size of the vocabulary in the data. We used a vocabulary size of 5000 (0 - 4999). Overall, we tested the vocabulary size between 5.000 – 10.000 words to reduce the parameters and improve the performance. Second, for the output_dim (output dimension) we defined a 32 dimensional vector space in which the words will be embedded. Third, the input_lenght of the input sequences was set to 500 because we will only consider a maximum review length of 500 words, due to the distribution of review lengths seen in the boxplot in the data exploration part. As CNNs expect a fixed size of input vectors and we set the input_lenght to 500 words, we have to either truncate longer reviews or pad shorter reviews with zeros at the end when loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ninosyonan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# first we import all the necessary packages for the tutorial\n",
    "    # for more information please check the helperfunction.py file\n",
    "\n",
    "from helperfunctions5 import *\n",
    "\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we initialize the random number generator to a constant value so that we can easily reproduce results\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 500)\n",
      "(25000, 500)\n",
      "Documents adjusted: all reviews have the same length of 500 words.\n"
     ]
    }
   ],
   "source": [
    "# load the imdb data set and sequence the dataset to a maximum review length of 500 words\n",
    "    # pad each document to ensure that they are of the same length of 500 for each observation\n",
    "    # longer sequences are truncated and shorter sequences are padded with zeros at the end\n",
    "# we will focus onlyon the first 5000 most used words in the dataset\n",
    "   \n",
    "top_words = 5000\n",
    "max_words = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_words)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_words)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(\"Documents adjusted: all reviews have the same length of\",max_words,\"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron Model\n",
    "\n",
    "One of the models we chose to predict the sentiment of the IMDB dataset is a multilayer perceptron model. It is a feedforward neural network.  It consists of three layers, the input layer, the hidden layer and the output layer. It is interconnected over neurons, each of them is connected to the neurons in the next layer. We used a “Sequential” model from Keras to build the MLP. Keras defines this model as a linear stack of layers (Keras Documentation). The embedding layer which was built build already in the previous chapter Word Embeddings served as the input layer. The next step was to flatten this layer to one dimension and afterwards to add the hidden layer with 250 units. A rectifier activation function was used in this part of the model. The last layer has an output of one neuron which is binary and can have the values 0 or 1 (positive or negative). It is activated with the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build MLP model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               4000250   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 4,160,501\n",
      "Trainable params: 4,160,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Build MLP model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# we used binary_crossentropy loss here because it is a binary classification problem\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 40s - loss: 0.5124 - acc: 0.7083 - val_loss: 0.3403 - val_acc: 0.8502\n",
      "Epoch 2/2\n",
      " - 33s - loss: 0.1925 - acc: 0.9265 - val_loss: 0.3006 - val_acc: 0.8738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2a50df98>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# running this snippet fits the model \n",
    "\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.38%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this  simple model achieved a score of nearly 87.38% which is very close to the original paper. But we can get more out of this. Therefore lets try another network model - the Concolutional Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "## Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/CNN.png\" alt=\"CNN_arc\" style=\"width: 500px;\"/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  CNN Model\n",
    "\n",
    "After performing a MLP, we conducted a one-dimensional convolutional neural network for the IMDB dataset. When looking at our dataset, an advantage for us is that Keras already provides the necessary one-dimensional convolutions as well as the Conv1D and MaxPooling1D classes. The first layer to start with is the embedding layer. After transforming the words into dense vectors they can be shifted to a convolutional layer, that with the help of the filters, indicates the proper sentiment in the sequence of words. \n",
    "\n",
    "In contrast to image pixels, in which the filters in the convolutional layer would slide over local patches, in NLP tasks, the filters would instead slide over full rows of the input matrix representing the words. Therefore, the width of the filters is equal compared to the width of the matrix in the embedding layer. Nevertheless, the height may vary but it is quite common to determine a sliding window of over two to five words at a time.\n",
    "\n",
    "Next, the pooling layer is applied right after the convolutional layer.  Generally speaking, max-pooling is the most common implementation. An advantage of the pooling layer is the outcome of a fixed size output matrix. This process is inevitable because in the end the output has to be fed into the classifier. Furthermore, the operation aggregates features over a region by calculating the maximum value of the features in the region. This means that it reduces the output dimensionality while still keeping the most salient information.\n",
    "\n",
    "Afterwards, the Flatten() operation takes the output and flattens the structure in order to create a single long feature vector, so that it can be used by the following dense layer for the final classification. The final dense layer with the activation function sigmoid transforms the output into a single output in order to indicate the sentiment.\n",
    "\n",
    "In the end we used the binary_crossentropy loss function for our binary classification problem. Again, the Adam optimization algorithm is performed, since it is known to be very fast, efficient and had become very popular in recent deep learning model applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build CNN model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 250)               2000250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 2,163,605\n",
      "Trainable params: 2,163,605\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Conv:\n",
    "    # filter: dimensionality of the output space (32 dimensions) \n",
    "    # kernel_size: reads (window) embedded word representations 3 vector elements of the word embedding at a time\n",
    "    # padding: \"same\" results in padding the input such that the output has the same length as the original input\n",
    "    # activation: activation function to use is 'relu': relu has better properties and speeds up the training\n",
    "# MaxPooling:\n",
    "    # pool_size: 2 the pooling layer is used to reduce the amount of parameters to simplify the computation\n",
    "# Flatten\n",
    "    # to connect a Dense layer directly to an Embedding layer, flatten the 2D output matrix to a 1D vector\n",
    "# Dense (sigmoid): sigmoid activation will produce a float number between 0 and 1\n",
    "\n",
    "print('Build CNN model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Conv1D(filters=32, \n",
    "                 kernel_size=3, \n",
    "                 padding='same', \n",
    "                 activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 38s - loss: 0.5144 - acc: 0.7116 - val_loss: 0.2850 - val_acc: 0.8810\n",
      "Epoch 2/2\n",
      " - 36s - loss: 0.2240 - acc: 0.9130 - val_loss: 0.2701 - val_acc: 0.8875\n"
     ]
    }
   ],
   "source": [
    "# epochs: it passed 2 times through the full training set\n",
    "# batch size: the number of training examples in one forward/backward pass\n",
    "# vebose: 2 = one line per epoch\n",
    "    # running the example (accuracy of 88.73%) offers a small improvement over the neural network model above\n",
    "\n",
    "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2, batch_size=128, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.75%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After two rounds we achieved quick a satisfactory outcome of 88.75% accuracy. Besides, it is an improvement compared to the end result of the MLP model we conducted earlier. Now there are a lot of opportunities to optimize and configure the model. You can play with the different settings and try to boost the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 10s 416us/step\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test)\n",
    "preds = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "Calculating a confusion matrix can give you a better idea of what your classification model is getting right and what types of errors it is making. The confusion matrix below, visualizes the outcome of the binary classification:\n",
    "\n",
    "* Total of 2813 wrong classifications and 22187 true classifications\n",
    "* True negative: 11093 | False positive: 1407\n",
    "* False negative: 1406 | True positive: 11094\n",
    "* Accuracy: 88,75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11093  1407]\n",
      " [ 1406 11094]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEmCAYAAACzoiEDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9g4nlzmXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a31866fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we want to setup and generate a cofusion matrix to have get a better understanding of the evaluation\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "   \n",
    "    plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    print(cm)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "\n",
    "# plot the confusion Matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "plot_confusion_matrix(cm, {'negative': 0, 'positive': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* #### In this tutorial, we discovered the topic of Sentiment Analysis with the Keras IMDB dataset.<br><br>\n",
    "\n",
    "* #### We learned how to develop deep learning models for sentiment analysis including:\n",
    "    * How to handle the basic dictionary approach for sentiment analysis<br><br>\n",
    "    * How to load review and analyze the IMDB dataset within Keras<br><br>\n",
    "    * How to use and build word embeddings with the Keras Embedding Layer for deep learning<br><br>\n",
    "    * How to develop a one-dimensional CNN model for sentiment analysis and how it works for NLP<br><br>\n",
    "    \n",
    "* #### How to continue with this tutorial?\n",
    "    * Try to experiment with the number of features such as filter size in the convolutional layer<br><br>\n",
    "    * You can also experiment with several convolutional layers and maxpooling layers, etc.<br><br>\n",
    "    * Try to obtain higher accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Limitations and further Topics\n",
    "\n",
    "* CNNs are not able to encode long-range dependencies, and therefore, for some language modeling tasks, where long-distance dependence matters, other  architectures are preferred:<br><br>\n",
    "    * Recurrent Neural Networks (RNN)<br><br>\n",
    "    * Long Short Term memory (LSTM) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
