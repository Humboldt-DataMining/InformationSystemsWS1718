{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis_Helperfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To keep the tutorial clean we created a helperfunction-file. This file contains all the functions that are needed for the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the all packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import wordcloud\n",
    "import csv\n",
    "import testfixtures\n",
    "import statsmodels\n",
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import SVG\n",
    "from IPython.display import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding, SpatialDropout1D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.data_utils import get_file\n",
    "from imp import reload\n",
    "from PIL import Image\n",
    "from itertools import chain\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.random import normal\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### Download the dataset and save it to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the dataset to a .csv file \n",
    "\n",
    "# create an index/word mapping, get full dataset copied from Keras and separate features from labels\n",
    "index = imdb.get_word_index()\n",
    "index2word = {v: k for k, v in index.items()}\n",
    "path = get_file('imdb_full.pkl',\n",
    "                origin='https://s3.amazonaws.com/text-datasets/imdb_full.pkl',\n",
    "                md5_hash='d091312047c43cf9e4e38fef92437263')\n",
    "\n",
    "# split into train and test \n",
    "f = open(path, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)\n",
    "\n",
    "# move the train data to csv\n",
    "with open('train.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(0, len(x_train)):\n",
    "        label = labels_train[i]\n",
    "        review = ' '.join([index2word[o] for o in x_train[i]])\n",
    "        writer.writerow([review, label])\n",
    "        \n",
    "# move the test data to csv\n",
    "with open('test.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(0, len(x_test)):\n",
    "        label = labels_test[i]\n",
    "        review = ' '.join([index2word[o] for o in x_test[i]])\n",
    "        writer.writerow([review, label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv', header=None)\n",
    "test_data = pd.read_csv('test.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with io.open('aclImdb/train-pos.txt', encoding='utf-8') as f:\n",
    "    train_pos = pd.DataFrame({'review': list(f)})    \n",
    "with io.open('aclImdb/train-neg.txt', encoding='utf-8') as f:\n",
    "    train_neg = pd.DataFrame({'review': list(f)}) \n",
    "train_reviews = pd.concat([train_neg, train_pos], ignore_index=True)\n",
    "\n",
    "with io.open('aclImdb/test-pos.txt', encoding='utf-8') as f:\n",
    "    test_pos = pd.DataFrame({'review': list(f)})\n",
    "with io.open('aclImdb/test-neg.txt', encoding='utf-8') as f:\n",
    "    test_neg = pd.DataFrame({'review': list(f)})    \n",
    "test_reviews = pd.concat([test_neg, test_pos], ignore_index=True)\n",
    "  \n",
    "X_train = train_reviews['review']\n",
    "X_test = test_reviews['review']\n",
    "\n",
    "y_train = np.append(np.zeros(12500), np.ones(12500))\n",
    "y_test = np.append(np.zeros(12500), np.ones(12500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression to classify which features are important.\n",
    "* ##### Results are shown in the Preprocessing-File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.03, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "stopwords_nltk = set(stopwords.words(\"english\"))\n",
    "relevant_words = set(['not', 'nor', 'no', 'wasn', 'ain', 'aren', 'very', 'only', 'but', 'don', 'isn', 'weren'])\n",
    "filtered_stopwords = list(stopwords_nltk.difference(relevant_words))\n",
    "vectorizer = CountVectorizer(stop_words =  filtered_stopwords, max_features = 10000, ngram_range = (1,2))\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "logistic_model = LogisticRegression(C=0.03) \n",
    "logistic_model.fit(X_train_features, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
