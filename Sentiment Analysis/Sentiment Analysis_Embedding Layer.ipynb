{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Keras definition says that word embeddings turn positive integers (indexes) into dense vectors of fixed size. Generally speaking, word embeddings is a technique in the field of NLP. It describes a technique where words are encoded as dense vectors in a high-dimensional space that carry a meaning. Each word has a specific position within the vector space. This position is learned from the text during the training and is premised on the surrounding words. Training the model will cause that semantically similar words will appear closer together also in the vector space.\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "\n",
    "A simple bag of words has for the same representation different meanings. Additionally, huge text datasets would be represented by a vector comprised by many zeros and only a few one’s which is not only inefficient but leads also to data sparsity.\n",
    "\n",
    "\n",
    "* “Netflix is better than Maxdome” \n",
    "    * Bag of words: [Netflix] [is] [better] [than] [Maxdome]<br><br>\n",
    "* One Hot Encoding: index of the specific word becomes a one, the rest becomes a zero\n",
    "    * Maxdome → 00001\n",
    "    * better → 00100\n",
    "    * Netflix → 10000\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding - Problems:\n",
    "1. Word ordering information is lost:\n",
    "    * Netflix is better than Maxdome vs. Maxdome is better than Netflix\n",
    "    * Different meaning but same representation<br><br>\n",
    "2. Data sparsity:\n",
    "    * many zeros and few ones (imagine 20.000 zeros)<br><br>\n",
    "3. Words as atomic symbols\n",
    "    * cat and dog would have the same distance as cat and apple\n",
    "    * but cats and dogs are closer together (both are animals)\n",
    "    * semantic similarity and relations is all learned from the data<br><br>\n",
    "    \n",
    "      <img src=\"images/WE1.png\" alt=\"WE\" style=\"width: 600px;\"/><br><br>\n",
    "    \n",
    "4. Very hard to find higher level features when using One Hot Encoding\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Below we want to show you an example why using dense vectors has a computational benefit when working with deep learning models such as CNNs. First, imagine you have the sentence \"deep learning is very deep\". Next, you have to decide on how long the vector should be (usally a lenght of 32 or 50). For this example we assign a lenghth of 6 factors per index in this post to keep it readable.\n",
    "\n",
    "\n",
    "<img src=\"images/Emb.png\" alt=\"Emb\" style=\"width: 350px;\"/>\n",
    "\n",
    "\n",
    "Now instead of ending up with huge one-hot encoded vectors, an embedding matrix keeps the size of a vector much smaller. The embedded vectors are learned during the training process. This is computationally efficient when using very big datasets. Below you see an example for the embedding matrix for the word deep:\n",
    "\n",
    "### deep = [.32, .02, .48, .21, .56, .15]<br><br>\n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vectors.png\" alt=\"vectors\" style=\"width: 600px;\"/><br><br>\n",
    "* Also relationships are learned, for example the information of gender<br><br>\n",
    "* Subtraction of vector [woman - man] is the same as [queen - king]\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings - Benefits:\n",
    "1. Trained in a completely unsupervised way, so you don't need labeled data <br><br>\n",
    "\n",
    "2. Reduce data sparsity because you are not dealing with a huge number of 0/1 you have a lot of float values<br><br>\n",
    "\n",
    "3. Semantic hashing\n",
    "    * representing the information of word meaning of the words\n",
    "    * semantic hashing: semantically similar words are closer <br><br>\n",
    "\n",
    "4. Freely available for out of the box usage\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Own Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a Nutshell:\n",
    "\n",
    "In the following you will see an example of how to learn a word embedding which is based on a neural network. This example aims to show how Keras supports word embeddings for deep learning in detail. First, it requires the input data to be integer encoded, so that each word is represented by a unique integer. Then the Embedding layer is initialized with random weights and will learn an embedding for all words in the training dataset. We will define a small problem with ten text documents and classify them as positive \"1\" or negative \"0\". We are using a Keras Sequential Model to finish the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the documents and classify them as positive “1” or negative “0”\n",
    "\n",
    "docs = ['good job', 'great work', 'well done', 'great effort', 'amazing',\n",
    "        'bad', 'poor work', 'very weak', 'could have done better', 'not good at all']\n",
    "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### One Hot Encoding\n",
    "\n",
    "Next, each document has to be integer encoded. This means that as input the Embedding layer will have sequences of integers. We use the Keras one_hot() function that creates a hash of each word as an efficient integer encoding. Further, the vocabulary size is estimated at 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13, 16], [19, 5], [16, 17], [19, 9], [9], [17], [12, 5], [13, 9], [17, 8, 17, 5], [7, 13, 18, 11]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the documents in docs\n",
    "\n",
    "embedding_vocab_size = 20\n",
    "encoded_docs = [one_hot(d, embedding_vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### Padding\n",
    "As Neural Networks are expecting a fixed size of input vector we have to pad each document to ensure they are of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[13 16  0  0]\n",
      " [19  5  0  0]\n",
      " [16 17  0  0]\n",
      " [19  9  0  0]\n",
      " [ 9  0  0  0]\n",
      " [17  0  0  0]\n",
      " [12  5  0  0]\n",
      " [13  9  0  0]\n",
      " [17  8 17  5]\n",
      " [ 7 13 18 11]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the documents in docs\n",
    "\n",
    "embedding_vocab_size = 20\n",
    "encoded_docs = [one_hot(d, embedding_vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### Keras Embedding Layer\n",
    "Finally, we will define our Embedding layer as part of a neural network model for binary classification.\n",
    "The Embedding has a vocabulary of 20 words and an input length of 4 words. We will choose a typical embedding space of 8 dimensions. Importantly, the output from the Embedding layer will be 4 vectors of 32 dimensions each, one for each word. In the end, we flatten this to a one 32-element vector to pass on to the Dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 32)             640       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 769\n",
      "Trainable params: 769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_vocab_size, \n",
    "                    output_dim=32, \n",
    "                    input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model shows that it learned the training dataset perfectly (which is not surprising)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
