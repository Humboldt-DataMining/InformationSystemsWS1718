{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Definition: “Turns positive integers (indexes) into dense vectors of fixed size”\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words\n",
    "* “Netflix is better than Maxdome” \n",
    "    * Bag of words: [Netflix] [is] [better] [than] [Maxdome]<br><br>\n",
    "* One Hot Encoding: index of the specific word becomes a one, the rest becomes a zero\n",
    "    * Maxdome → 00001\n",
    "    * better → 00100\n",
    "    * Netflix → 10000\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems:\n",
    "1. Word ordering information is lost:\n",
    "    * Netflix is better than Maxdome vs. Maxdome is better than Netflix\n",
    "    * Different meaning but same representation<br><br>\n",
    "2. Data sparsity:\n",
    "    * many zeros and few ones (imagine 20.000 zeros)<br><br>\n",
    "3. Words as atomic symbols\n",
    "    * cat and dog have the same distance as cat and apple\n",
    "    * but cats and dogs are closer together (both are animals)<br><br>\n",
    "4. Very hard to find higher level features\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "<img src=\"images/Emb.png\" alt=\"Emb\" style=\"width: 350px;\"/>\n",
    "* Create the embedding matrix next<br><br>\n",
    "* Decide on how long the vector should be (usually lenght of 32 or 50)<br><br>\n",
    "* We assign a lenghth of 6 factors per index in this post to keep it readable<br><br>\n",
    "\n",
    "___\n",
    "### The embedding matrix for the word deep:\n",
    "* ### deep = [.32, .02, .48, .21, .56, .15]<br><br>\n",
    "\n",
    "* Instead of ending up with huge one-hot encoded vectors, an embedding matrix keeps the size of a vector much smaller<br><br>\n",
    "* The embedded vectors are learned during the training process<br><br>\n",
    "* This is computationally efficient when using very big datasets\n",
    "\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings\n",
    "<img src=\"images/WE1.png\" alt=\"WE\" style=\"width: 600px;\"/>\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/collobert.png\" alt=\"collobert\" style=\"width: 600px;\"/>\n",
    "* Example of closest words found in a word embedding space\n",
    "* France → european countries, Jesus → close to religious words\n",
    "* It is all learned from the data, that these words are semantically similar and relations\n",
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap\n",
    "<img src=\"images/vectors.png\" alt=\"vectors\" style=\"width: 600px;\"/><br><br>\n",
    "* Also relationships are learned, for example the information of gender<br><br>\n",
    "* Remember: subtraction of vector [woman - man] is the same as [queen - king]\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits\n",
    "1 Trained in a completely unsupervised way\n",
    "* you don't need labeled data<br><br>\n",
    "\n",
    "2 Reduce data sparsity\n",
    "* you are not dealing with a huge number of 0/1 you have a lot of float values<br><br>\n",
    "\n",
    "3 Semantic hashing\n",
    "* representing the information of word meaning of the words\n",
    "* semantic hashing: semantically similar words are closer\n",
    "* Appear to carry semantic information about the words<br><br>\n",
    "\n",
    "4 Freely available for out of the box usage\n",
    "\n",
    "______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Your Own Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In a Nutshell:\n",
    "* Word embeddings provide a dense representation of words and their relative meanings<br><br>\n",
    "\n",
    "* They are an improvement over sparse representations used in simpler bag of word model representations<br><br>\n",
    "\n",
    "* Word embeddings can be learned from text data and reused among projects<br><br>\n",
    "\n",
    "* They can also be learned as part of fitting a neural network on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to train your own embedding layer in Keras\n",
    "\n",
    "* It requires the input data to be integer encoded, so that each word is represented by a unique integer<br><br>\n",
    "* Embedding layer is initialized with random weights and will learn an embedding for all words in the training dataset<br><br>\n",
    "* We will define a small problem with ten text documents and classify them as positive \"1\" or negative \"0\"<br><br>\n",
    "* We are using a Keras Sequential Model here to finish the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/ninosyonan/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import re\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the documents and classify them as positive “1” or negative “0”\n",
    "\n",
    "docs = ['well done', 'good work', 'great effort', 'nice work', 'excellent',\n",
    "        'weak', 'poor effort', 'not good', 'poor work', 'could have done better']\n",
    "labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### One Hot Encoding\n",
    "* Next, we can integer encode each document. This means that as input the Embedding layer will have sequences of integers<br><br>\n",
    "\n",
    "* We use the Keras one_hot() function that creates a hash of each word as an efficient integer encoding<br><br>\n",
    "\n",
    "* We will estimate the vocabulary size of 30, to reduce the probability of collisions from the hash function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17, 13], [5, 8], [10, 7], [10, 8], [6], [5], [4, 7], [15, 5], [4, 8], [2, 13, 13, 7]]\n"
     ]
    }
   ],
   "source": [
    "# one-hot encode the documents in docs\n",
    "\n",
    "embedding_vocab_size = 20\n",
    "encoded_docs = [one_hot(d, embedding_vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### Padding\n",
    "* Pad each document to ensure they are of the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17 13  0  0]\n",
      " [ 5  8  0  0]\n",
      " [10  7  0  0]\n",
      " [10  8  0  0]\n",
      " [ 6  0  0  0]\n",
      " [ 5  0  0  0]\n",
      " [ 4  7  0  0]\n",
      " [15  5  0  0]\n",
      " [ 4  8  0  0]\n",
      " [ 2 13 13  7]]\n"
     ]
    }
   ],
   "source": [
    "# pad documents to a max length of 4 words\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "#### Keras Embedding Layer\n",
    "* We will define our Embedding layer as part of a neural network model for binary classification\n",
    "\n",
    "* The Embedding has a vocabulary of 30 words and an input length of 4 words\n",
    "\n",
    "* We will choose a typical embedding space of 8 dimensions\n",
    "\n",
    "* Importantly, the output from the Embedding layer will be 4 vectors of 32 dimensions each, one for each word\n",
    "\n",
    "* We flatten this to a one 32-element vector to pass on to the Dense output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 4, 32)             640       \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 769\n",
      "Trainable params: 769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=embedding_vocab_size, \n",
    "                    output_dim=32, \n",
    "                    input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result: the trained model shows that it learned the training dataset perfectly (which is not surprising)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
