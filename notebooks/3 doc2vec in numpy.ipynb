{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:46:10.998608Z",
     "start_time": "2018-01-17T00:45:57.217022Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from random import randint\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:46:11.095392Z",
     "start_time": "2018-01-17T00:46:11.061369Z"
    }
   },
   "outputs": [],
   "source": [
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "data_dir = os.path.join(parent_dir, 'data')\n",
    "models_dir = os.path.join(parent_dir, 'models')\n",
    "print('working directory: ', os.getcwd())\n",
    "print('data directory:    ', data_dir, )\n",
    "print('models directory:  ', models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:46:11.898350Z",
     "start_time": "2018-01-17T00:46:11.132418Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(os.path.join(data_dir, 'data_clean_4cols.pickle'))\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:46:18.122436Z",
     "start_time": "2018-01-17T00:46:17.535195Z"
    }
   },
   "outputs": [],
   "source": [
    "candidate_data = pd.DataFrame()\n",
    "candidate_data = (data[['Partei_ABK', 'from_name']].drop_duplicates('from_name')\n",
    "                                                   .set_index('from_name'))\n",
    "# join all messages by the same candidate\n",
    "candidate_data['messages'] = data.groupby('from_name')['message'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:46:20.530622Z",
     "start_time": "2018-01-17T00:46:20.258429Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:47:39.686750Z",
     "start_time": "2018-01-17T00:47:39.676743Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "sample = candidate_data.sample(n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:47:50.538187Z",
     "start_time": "2018-01-17T00:47:50.508164Z"
    }
   },
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer = TweetTokenizer\n",
    "stopword_set = set(stopwords.words('german'))\n",
    "#This function does all cleaning of data using two objects above\n",
    "def nlp_clean(messages):\n",
    "    new_data = []\n",
    "    for message in messages:\n",
    "        message = message.lower()\n",
    "        message = tokenizer().tokenize(message)\n",
    "        words = [word for word in message if (word not in stopword_set\n",
    "                                              and word not in string.punctuation\n",
    "                                              and not word.startswith('http'))]\n",
    "        #word_list = tokenizer.tokenize(message)\n",
    "        #word_list = list(set(dlist).difference(stopword_set))\n",
    "        new_data.append(words)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:24:16.535999Z",
     "start_time": "2018-01-17T01:24:16.525990Z"
    }
   },
   "outputs": [],
   "source": [
    "a = 'aaaaa'\n",
    "not a.startswith('aa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:07:36.498600Z",
     "start_time": "2018-01-17T01:07:34.162875Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = nlp_clean(sample['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:10:41.952951Z",
     "start_time": "2018-01-17T01:10:41.939940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose a random document\n",
    "i = randint(0, n-1)\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:10:43.639343Z",
     "start_time": "2018-01-17T01:10:43.626335Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = np.array(np.zeros(n), ndmin=2).T\n",
    "d[i] = 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:11:24.919439Z",
     "start_time": "2018-01-17T01:11:24.899425Z"
    }
   },
   "outputs": [],
   "source": [
    "sample['messages'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:11:27.661043Z",
     "start_time": "2018-01-17T01:11:27.618012Z"
    }
   },
   "outputs": [],
   "source": [
    "documents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:50:39.012869Z",
     "start_time": "2018-01-17T00:50:38.946842Z"
    }
   },
   "outputs": [],
   "source": [
    "word_list = [word for words in documents for word in words]\n",
    "vocab = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:50:41.921790Z",
     "start_time": "2018-01-17T00:50:41.913784Z"
    }
   },
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:50:46.459282Z",
     "start_time": "2018-01-17T00:50:46.451277Z"
    }
   },
   "outputs": [],
   "source": [
    "m = len(vocab)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T12:22:02.029470Z",
     "start_time": "2018-01-15T12:22:01.982937Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:55:20.203124Z",
     "start_time": "2018-01-17T00:55:20.195136Z"
    }
   },
   "outputs": [],
   "source": [
    "target_word = 'steuern'\n",
    "idx = vocab.index(target_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T00:55:21.473885Z",
     "start_time": "2018-01-17T00:55:21.457875Z"
    }
   },
   "outputs": [],
   "source": [
    "t = np.array(np.zeros(m), ndmin=2).T\n",
    "t[vocab.index(target_word)] = 1\n",
    "t[idx-5:idx+5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T12:22:02.163065Z",
     "start_time": "2018-01-15T12:22:02.127540Z"
    }
   },
   "source": [
    "def get_window(i, window_size=8):\n",
    "    words = cleaned[i]\n",
    "    middle = randint(window_size, len(words) - window_size - 1)\n",
    "    index = np.zeros[len(words)]\n",
    "    for c in range(middle - window_size, middle + window_size):\n",
    "        if not c = middle:\n",
    "            index[c] = 1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T01:52:42.797903Z",
     "start_time": "2018-01-17T01:52:42.687826Z"
    }
   },
   "outputs": [],
   "source": [
    "# use this instead of get_window()\n",
    "window_size=8\n",
    "words = documents[i]\n",
    "middle = randint(window_size, len(words) - window_size - 1)\n",
    "#window_words = [words[c] for c in range(middle - window_size, middle + window_size)]\n",
    "#print(window_words)\n",
    "\n",
    "window_words = []\n",
    "k = []\n",
    "for c in range(middle - window_size, middle + window_size):\n",
    "    window_words.append(words[c])\n",
    "    kk = (np.array(np.zeros(m), ndmin=2).T)\n",
    "    kk[vocab.index(words[c])] = 1\n",
    "    k.append(kk)\n",
    "k[-1][vocab.index(words[c])-5:vocab.index(words[c])+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:06:09.607431Z",
     "start_time": "2018-01-17T02:06:09.543385Z"
    }
   },
   "outputs": [],
   "source": [
    "window_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:22.641241Z",
     "start_time": "2018-01-17T02:11:22.631244Z"
    }
   },
   "outputs": [],
   "source": [
    "# p = dimensions of document vectors (no. of features)\n",
    "p = 100\n",
    "D = np.random.rand(p, n)\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:24.386894Z",
     "start_time": "2018-01-17T02:11:24.130282Z"
    }
   },
   "outputs": [],
   "source": [
    "U = np.random.rand(m, p)\n",
    "U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:25.398404Z",
     "start_time": "2018-01-17T02:11:25.389398Z"
    }
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)  # axis=1 for row-vector, axis=0 for column-vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:26.430223Z",
     "start_time": "2018-01-17T02:11:26.413209Z"
    }
   },
   "outputs": [],
   "source": [
    "e = np.array(np.dot(D, d), ndmin=2)\n",
    "l = np.array(np.dot(U, e), ndmin=2)\n",
    "k_hat = softmax(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:27.473986Z",
     "start_time": "2018-01-17T02:11:27.462979Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:28.390755Z",
     "start_time": "2018-01-17T02:11:28.381749Z"
    }
   },
   "outputs": [],
   "source": [
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:29.460761Z",
     "start_time": "2018-01-17T02:11:29.448751Z"
    }
   },
   "outputs": [],
   "source": [
    "k_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:30.323372Z",
     "start_time": "2018-01-17T02:11:30.313363Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(k_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:31.486613Z",
     "start_time": "2018-01-17T02:11:31.477608Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(k, k_hat):\n",
    "    return - np.dot(k.T, np.log(k_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:32.401859Z",
     "start_time": "2018-01-17T02:11:32.380252Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_entropy_loss(k[0], k_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:42.696567Z",
     "start_time": "2018-01-17T02:11:42.687560Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "errors_out = k_hat - k[0]\n",
    "#EI = np.dot(t.T, errors)\n",
    "#EI\n",
    "errors_out#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:44.487948Z",
     "start_time": "2018-01-17T02:11:44.477940Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_middle = np.dot(U.T, errors_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:45.639248Z",
     "start_time": "2018-01-17T02:11:45.631242Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:46.737534Z",
     "start_time": "2018-01-17T02:11:46.601566Z"
    }
   },
   "outputs": [],
   "source": [
    "U += - alpha * np.dot(errors_out, e.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:11:47.647337Z",
     "start_time": "2018-01-17T02:11:47.636329Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "D += - alpha * np.dot(errors_middle, d.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=8\n",
    "alpha_start = 0.025\n",
    "alpha_end = 0.0001\n",
    "p = 100  # p = dimensions of document vectors (no. of features)\n",
    "m = len(vocab)\n",
    "\n",
    "D = np.random.rand(p, n)\n",
    "U = np.random.rand(m, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:27:44.597790Z",
     "start_time": "2018-01-17T02:27:44.583778Z"
    }
   },
   "outputs": [],
   "source": [
    "np.array(, ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:40:37.517357Z",
     "start_time": "2018-01-17T02:36:46.259246Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for e in range(epochs):\n",
    "    for i in range(n):\n",
    "        d = np.array(np.zeros(n), ndmin=2).T\n",
    "        d[i] = 1\n",
    "        \n",
    "        e = np.array(np.dot(D, d), ndmin=2)\n",
    "        l = np.array(np.dot(U, e), ndmin=2)\n",
    "        k_hat = softmax(l)\n",
    "        \n",
    "        #window_size=8\n",
    "        doc_words = documents[i]\n",
    "        middle = randint(window_size, len(doc_words) - window_size - 1)\n",
    "        #window_words = [words[c] for c in range(middle - window_size, middle + window_size)]\n",
    "        #t[] = np.array(np.zeros(len(vocab)), ndmin=2).T\n",
    "        #for w in window_words:\n",
    "        #    t[vocab.index(w)] = 1\n",
    "\n",
    "        #window_words = []\n",
    "        #k = []\n",
    "        errors_out = (np.array(np.zeros(m), ndmin=2).T)\n",
    "        errors_middle = (np.array(np.zeros(p), ndmin=2).T)\n",
    "        for c in range(middle - window_size, middle + window_size):\n",
    "            #window_word = doc_words[c]\n",
    "            k = (np.array(np.zeros(len(vocab)), ndmin=2).T)\n",
    "            k[vocab.index(doc_words[c])] = 1\n",
    "            #k.append(kk)\n",
    "            errors_out += k_hat - k\n",
    "            errors_middle += np.dot(U.T, errors_out)\n",
    "            \n",
    "            if c == middle:\n",
    "                print(cross_entropy_loss(k, k_hat))\n",
    "\n",
    "        errors_out = k_hat - k\n",
    "        errors_middle = np.dot(U.T, errors_out)\n",
    "        U += - alpha * np.dot(errors_out, e.T)\n",
    "        D += - alpha * np.dot(errors_middle, d.T)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:33:44.184079Z",
     "start_time": "2018-01-17T02:33:41.323736Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "tsne = TSNE(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:33:45.412013Z",
     "start_time": "2018-01-17T02:33:45.389999Z"
    }
   },
   "outputs": [],
   "source": [
    "party_colors = {'AfD': 'xkcd:blue',\n",
    "                'DIE LINKE': 'xkcd:magenta',\n",
    "                'GRÜNE': 'xkcd:grass green',\n",
    "                'CSU': 'xkcd:sky blue',\n",
    "                'CDU': 'xkcd:black',\n",
    "                'FDP': 'xkcd:goldenrod',\n",
    "                'SPD': 'xkcd:red'}\n",
    "sample['color'] = sample['Partei_ABK'].map(party_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:33:50.783744Z",
     "start_time": "2018-01-17T02:33:48.183268Z"
    }
   },
   "outputs": [],
   "source": [
    "D_tsne = tsne.fit_transform(D)\n",
    "plt.figure(num=None, figsize=(10, 8))  # set the figure size\n",
    "plt.scatter(D_tsne[:, 0], D_tsne[:, 1], c=sample['color'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:36:05.325393Z",
     "start_time": "2018-01-17T02:36:02.843472Z"
    }
   },
   "outputs": [],
   "source": [
    "D_tsne = tsne.fit_transform(D)\n",
    "plt.figure(num=None, figsize=(10, 8))  # set the figure size\n",
    "plt.scatter(D_tsne[:, 0], D_tsne[:, 1], c=sample['color'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T02:51:42.317743Z",
     "start_time": "2018-01-17T02:51:39.066522Z"
    }
   },
   "outputs": [],
   "source": [
    "D_tsne = tsne.fit_transform(D)\n",
    "plt.figure(num=None, figsize=(10, 8))  # set the figure size\n",
    "plt.scatter(D_tsne[:, 0], D_tsne[:, 1], c=sample['color'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T14:39:28.591635Z",
     "start_time": "2018-01-15T14:39:27.606258Z"
    }
   },
   "outputs": [],
   "source": [
    "D_tsne = tsne.fit_transform(D)\n",
    "plt.figure(num=None, figsize=(10, 8))  # set the figure size\n",
    "plt.scatter(D_tsne[:, 0], D_tsne[:, 1], c=sample['color'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-15T15:26:51.949557Z",
     "start_time": "2018-01-15T15:26:51.941553Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "100 * (100 - 5) // 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T03:15:11.405467Z",
     "start_time": "2018-01-17T03:14:57.428159Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T03:17:48.248410Z",
     "start_time": "2018-01-17T03:17:48.040496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "dataset = tf.placeholder(tf.int32, shape=[80])\n",
    "labels = tf.placeholder(tf.int32, shape=[80, 1])\n",
    "\n",
    "# Weights\n",
    "embeddings = tf.Variable(\n",
    "        tf.random_uniform([n, p],\n",
    "                          -1.0, 1.0))\n",
    "softmax_weights = tf.Variable(\n",
    "        tf.truncated_normal(\n",
    "                [m, p],\n",
    "                stddev=1.0 / np.sqrt(p)))\n",
    "softmax_biases = tf.Variable(tf.zeros([m]))\n",
    "\n",
    "# Model\n",
    "# Look up embeddings for inputs\n",
    "embed = tf.nn.embedding_lookup(embeddings, dataset)\n",
    "# Compute the softmax loss, using a sample of the negative\n",
    "# labels each time\n",
    "loss = tf.reduce_mean(\n",
    "        tf.nn.sampled_softmax_loss(\n",
    "                softmax_weights, softmax_biases, embed,\n",
    "                labels, 64, m))\n",
    "\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdagradOptimizer(alpha).minimize(loss)\n",
    "\n",
    "# Test loss\n",
    "test_loss = tf.reduce_mean(\n",
    "        tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                tf.matmul(embed, tf.transpose(\n",
    "                          softmax_weights)) + softmax_biases,\n",
    "                labels[:, 0]))\n",
    "\n",
    "# Normalized embeddings (to use cosine similarity later on)\n",
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1,\n",
    "                              keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
