Neural networks are being alternatively used instead of the traditional statistical models to solve prediction and classification problems. According to \cite{nielsen2015neural}, the neural networks are being more efficient in providing the best solutions to problems such as speech recognition, image recognition and natural language processing. It is also proved by various researchers that the neural network model turns out to be more effective compared to other models in data classification (\cite{savchenko2013real}). \cite{gallinari1991relations} have shown analytical results which illustrate the relations between discriminant analysis and multilayer perceptrons used to solve classification problems. A similar comparison of statistical methods with neural networks has been done by \cite{cheng1994neural}. According to \cite{dreiseitl2002logistic}, the neural networks and traditional statistical methods have been the most widely used models in biomedicine. \cite{warner1996understanding} also illustrated how neural nets performs as a nonparametric regression model with the power to model complex functional forms. Moreover, they argue that despite the genuine intention to develop a neural network by modeling the human brain, the current neural networks  have little to do with what the biological neurons do. 

Despite their power to predict and classify, neural nets are prone to errors. \cite{wilamowski2009neural} warn about the number of the nodes in the hidden layers. They argue that if too many nodes are used within the layers of network, the network will be overtrained on the training pattern and can no longer recognize the new patterns. They further claim that although a larger number of nodes will give a better generalization, a small number of nodes will provide better approximation for the new patterns. \cite{dreiseitl2002logistic} claims that it is enough to have one hidden layer to classify most data sets. However, when it comes to the number of nodes in the hidden layers, they opt for empirical results by cross-validation or bootstrapping. \cite{hansen1990neural} implements the tool of cross-validation to optimize the neural network parameters.

Cross-validation refers to the process of evaluating the performance of neural network by using a fraction of the dataset in training against the rest of the dataset and ultimately measuring the capacity of the network in order to make a generalization (\cite{hansen1990neural}). \cite{toussaint1974bibliography} defines cross-validation as a standard tool to decide among different parametrized choices for a dataset estimated using the traditional statistical methods. However, \cite{rumelhart1985learning} assert that the standard way to train a neural network is to train on the whole dataset in order to minimize the aggregate error of misclassification in the dataset. \cite{srivastava2014dropout} introduced the dropout technique to tackle the overfitting problem when the number of parameters is large. The technique is to randomly drop nodes with their connections from the network throughout the training process which will result in much lower co-adaption of the nodes.

\cite{ruder2016overview} gives a gentle comparison of different variants of gradient descent, the most commonly used optimizer for the neural networks. In his paper, he compares the batch gradient descent, stochastic gradient descent and the mini-batch gradient descent. The batch gradient descent is used to update the parameters for the entire training data. It is slow and cannot estimate the gradient for new observations during the training.  However, Ruder argues that it converges to the global minimum of the cost function if the surface is convex and converges to the local minimum for non-convex surfaces. The stochastic gradient descent, in contrast, estimates the gradient for a single randomly-selected observation in the training dataset in each iteration (\cite{bottou2012stochastic}). Bottou also claims that the stochastic gradient descent is able to estimate the gradient for the new observations during the training as opposed to the batch gradient descent. He concludes that whenever time is an issue and the data is abundant (\cite{bottou2010large}), use the stochastic gradient descent. There is a trade-off between the time of parameter update and its accuracy (\cite{ruder2016overview}). The third variant of gradient descent is the mini-batch gradient descent which estimates parameters for every mini-batch of the training observations and falls between the stochastic gradient descent and batch gradient descent. \cite{shalev2013accelerated} state that the mini-batch has gained popularity in recent years and they introduce several popular papers such as Dekel et al 2012 , Takac et al 2013, Fercoq and Richtarik 2013 and a few more who used mini-batch as the method of choice. \cite{ruder2016overview} also states that mini-batch gradient descent is the method of choice nowadays and that the term SGD (stochastic Gradient Descent) is also used when the method is mini-batch.

It is possible that the gradient converges to a local minimum instead of a global minimum during the training process. However, there are two adjustable parameters which will help to avoid the trap of local minima (\cite{sibanda2012artificial}). \cite{riedmiller1993direct} claim that the choice of learning rate will affect the convergence timing. They further argue that a small learning rate will take a considerable amount of time to converge to an acceptable value while setting the learning rate large may lead to oscillation and fail to reach the global minimum. \cite{bottou2012stochastic} advises to play with the learning rate by using a small fraction of the training dataset. Interestingly enough, \cite{ruder2016overview} shows that by dropping the learning rate slowly, the convergence of stochastic gradient descent and batch gradient descent coincide, assuring a local minimum for non-convex and a global minimum for convex surfaces. To reduce the effect of high oscillations around the local optima in order to avoid the trap of local minimum convergence, the momentum parameter is used (\cite{rumelhart1986learning}). It is believed that using momentum will stabilize the learning process and will speed up the convergence (\cite{riedmiller1993direct}).

The feedforward pass, backpropagating the error and parameters update are all key processes which make a neural network work efficiently. In what follows, the reader will find an easy step-by-step guide on how to implement all these processes from scratch, which draws inspiration from  the works of \cite{dima}, \cite{nielsen2015neural} and \cite{rashid2016make}.