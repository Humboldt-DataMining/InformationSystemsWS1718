{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Neural Network Fundamentals. Gentle Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Intro: Imagine the following problem.\n",
    "There are handwritten numbers that you want computer to correctly clasify. It would be an easy task for a person but an extremely complicated one for a machine, especially, if you want to use some traditional prediction model, like linear regression. Even though the computer is faster than the human brain in numeric computations, the brain far outperforms the computer in some tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/problem.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Some intuition from the Nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People struggled to teach machines to solve this kind of problems for a long time without success.\n",
    "Unless they noticed a very peculiar thing. Nature creatures, even the simple ones, for instance \n",
    "insects, can perform complicated task with very limited brain capacities, which are far below \n",
    "those of the computers. So there is something nature has developed that aloows to solve tasks \n",
    "apparently complicated for machines tasks in a smart way.\n",
    "One of the ides that came to mind is to replicate the structure and certain functions of nature beigns \n",
    "brain and neurosystem that allow for cognitive procecess and beyond.\n",
    "A particular example of such structures is neuron system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neurons.png\" alt=\"Drawing\" style=\"width: 600px;\"/> [Source: https://www.jstor.org/stable/pdf/2684922.pdf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neurons_net3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> [Source: https://pixabay.com/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular detail about how are our cognitive and perceptive processes organised is a complicated \n",
    "structure of simple elements which create a complex net where each element is connected with othere \n",
    "receiving and transmitting information. An idea to reemplement such a structure in order to make \n",
    "predictions gave birth to what we now now as neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early development:\n",
    "##### 1943\n",
    "McCulloch-Pitts model of the neuron. The neuron receives a weighted sum of inputs from connected units, and outputs a value of one (fires) if this sum is greater than a thresh- old. If the sum is less than the threshold, the model neuron outputs a zero value.\n",
    "\n",
    "##### Early 1960s\n",
    "Rosenblatt developed a model called simple perceptron. The simple perceptron consists of McCulloch- Pitts model neurons that form two layers, input and output. His model was able to find a solution to classification problems if the problem was linearly separable.\n",
    "Later on Minsky and Papert addressed the linear severability limitation of Rosenblatt model. He knew it himself but could not figure it out.\n",
    "This hindered the process of NNs development.\n",
    "\n",
    "##### 1982\n",
    "Hopfield Model used mainly for optimization problems like travel sales man problem\n",
    "Later on, the idea of backpropagation was introduced and it addressed the earlier problems of the simple perceptron and renewed interest in neural networks. Backpropagation training algorithm is capable of solving nonlinear separable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differnt Applications\n",
    "At the current stage NNs are capable to model many of the capabilities of the human brain and beyond.\n",
    "On a practical level the human brain has many features that are desirable in an electronic computer. The human brain has the ability to generalize from abstract ideas, recognize patterns in the presence of noise, quickly recall memories, and withstand localized damage.\n",
    "\n",
    "Usages of NNS:\n",
    "- identifying underwater sonar contacts\n",
    "- predicting heart problems in patients\n",
    "- diagnosing hypertension\n",
    "- recognizing speech\n",
    "- the preferred tool in predicting protein secondary structures\n",
    "\n",
    "Staticians use these models to address the same problems:\n",
    "- discriminant analysis\n",
    "- logistic regression\n",
    "- Bayes and other types of classifiers\n",
    "- multiple regression\n",
    "- time series models such as ARIMA and other forecasting methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schematic Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the aplications of neural networks mentioned above have in common a simlified structure depicted on the following picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the NN from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to reimlement such a structure using Python. \n",
    "\n",
    "The crutial elements are:\n",
    "* layers\n",
    "* nodes\n",
    "* weights between them\n",
    "* activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to work with numbers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the structure of the NN\n",
    "i_n = 3\n",
    "h_n = 5\n",
    "o_n = 2\n",
    "# i_n, h_n and o_n stand here for the number of nodes in input, hidden and output layers respectively \n",
    "# - exactly as on the picture above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16320867,  0.27111435,  0.05803869],\n",
       "       [ 0.60824488,  0.06341481,  0.38990072],\n",
       "       [ 0.71077665,  0.23453235,  0.42572539],\n",
       "       [ 0.91378956,  0.84418591,  0.52465855],\n",
       "       [ 0.23073563,  0.84500162,  0.62394   ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly define the weights between nodes \n",
    "w_i_h = np.random.rand(h_n, i_n)\n",
    "w_h_o = np.random.rand(o_n, h_n)\n",
    "\n",
    "# Show matrices of randomly assigned weights\n",
    "w_i_h\n",
    "# w_h_o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides complicated multilayer structure with many nodes neurosystems in nature has one more important feature - neurons in them send signal further or \"fire\" only when they get a signal that is strong enough - stronger than certain treshold. This can be represented by a step function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step_function.png\" alt=\"Drawing\" style=\"width: 700px;\"/> [Source: https://www.researchgate.net/figure/Three-different-types-of-transfer-function-step-sigmoid-and-linear-in-unipolar-and_306323136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine activation function which is an approximation for \"firing\" of neurons\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111ce7588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw this function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like all the elements of our neural network structure are ready. Can we use this structure to tackle the problem?\n",
    "\n",
    "Let's take a look at our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = open(\"data/mnist_train_100.csv\", 'r')\n",
    "data = raw_data.readlines()\n",
    "raw_data.close()\n",
    "\n",
    "# The sourse of the data can be found here\n",
    "# https://pjreddie.com/projects/mnist-in-csv/\n",
    "# http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data - check the number of observations\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,18,18,18,126,136,175,26,166,255,247,127,0,0,0,0,0,0,0,0,0,0,0,0,30,36,94,154,170,253,253,253,253,253,225,172,253,242,195,64,0,0,0,0,0,0,0,0,0,0,0,49,238,253,253,253,253,253,253,253,253,251,93,82,82,56,39,0,0,0,0,0,0,0,0,0,0,0,0,18,219,253,253,253,253,253,198,182,247,241,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,156,107,253,253,205,11,0,43,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,1,154,253,90,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,253,190,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,190,253,70,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,35,241,225,160,108,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,81,240,253,253,119,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,45,186,253,253,150,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,16,93,252,253,187,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,249,253,249,64,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,130,183,253,253,207,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,39,148,229,253,253,253,250,182,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,114,221,253,253,253,253,201,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,66,213,253,253,253,253,198,81,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,171,219,253,253,253,253,195,80,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,55,172,226,253,253,253,253,244,133,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,136,253,253,253,212,135,132,16,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a particular observation of the data and explain it\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to plot the data\n",
    "import matplotlib.pyplot as mpp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x114bbb358>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4QeCIvxAUP8PRZ8Vlgh2BcUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111ccbc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "observation = data[0].split(',')\n",
    "image = np.asfarray(observation[1:]).reshape((28,28))\n",
    "mpp.imshow(image, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save an observation of the data as an input to work with.\n",
    "\n",
    "inputs = np.array(np.asfarray(observation[1:]), ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   3.],\n",
       "       [  18.],\n",
       "       [  18.],\n",
       "       [  18.],\n",
       "       [ 126.],\n",
       "       [ 136.],\n",
       "       [ 175.],\n",
       "       [  26.],\n",
       "       [ 166.],\n",
       "       [ 255.],\n",
       "       [ 247.],\n",
       "       [ 127.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  30.],\n",
       "       [  36.],\n",
       "       [  94.],\n",
       "       [ 154.],\n",
       "       [ 170.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 225.],\n",
       "       [ 172.],\n",
       "       [ 253.],\n",
       "       [ 242.],\n",
       "       [ 195.],\n",
       "       [  64.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  49.],\n",
       "       [ 238.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 251.],\n",
       "       [  93.],\n",
       "       [  82.],\n",
       "       [  82.],\n",
       "       [  56.],\n",
       "       [  39.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  18.],\n",
       "       [ 219.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 198.],\n",
       "       [ 182.],\n",
       "       [ 247.],\n",
       "       [ 241.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  80.],\n",
       "       [ 156.],\n",
       "       [ 107.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 205.],\n",
       "       [  11.],\n",
       "       [   0.],\n",
       "       [  43.],\n",
       "       [ 154.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  14.],\n",
       "       [   1.],\n",
       "       [ 154.],\n",
       "       [ 253.],\n",
       "       [  90.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [ 139.],\n",
       "       [ 253.],\n",
       "       [ 190.],\n",
       "       [   2.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  11.],\n",
       "       [ 190.],\n",
       "       [ 253.],\n",
       "       [  70.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  35.],\n",
       "       [ 241.],\n",
       "       [ 225.],\n",
       "       [ 160.],\n",
       "       [ 108.],\n",
       "       [   1.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  81.],\n",
       "       [ 240.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 119.],\n",
       "       [  25.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  45.],\n",
       "       [ 186.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 150.],\n",
       "       [  27.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  16.],\n",
       "       [  93.],\n",
       "       [ 252.],\n",
       "       [ 253.],\n",
       "       [ 187.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [ 249.],\n",
       "       [ 253.],\n",
       "       [ 249.],\n",
       "       [  64.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  46.],\n",
       "       [ 130.],\n",
       "       [ 183.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 207.],\n",
       "       [   2.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  39.],\n",
       "       [ 148.],\n",
       "       [ 229.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 250.],\n",
       "       [ 182.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  24.],\n",
       "       [ 114.],\n",
       "       [ 221.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 201.],\n",
       "       [  78.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  23.],\n",
       "       [  66.],\n",
       "       [ 213.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 198.],\n",
       "       [  81.],\n",
       "       [   2.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  18.],\n",
       "       [ 171.],\n",
       "       [ 219.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 195.],\n",
       "       [  80.],\n",
       "       [   9.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [  55.],\n",
       "       [ 172.],\n",
       "       [ 226.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 244.],\n",
       "       [ 133.],\n",
       "       [  11.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [ 136.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 253.],\n",
       "       [ 212.],\n",
       "       [ 135.],\n",
       "       [ 132.],\n",
       "       [  16.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.],\n",
       "       [   0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fit draft of the NN to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have 784 elements in our inputs. Therefore it would be logical to update the structure of the neural network. Instead of 3 input nodes we will have 784. Accordinly, as we have 10 different options for the outcome *from 0 to 9) we schould better have 10 output nodes instead of 1. 100 nodes in the hidden layer have been assigned as something in between 784 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the structure of the NN\n",
    "i_n = 784\n",
    "h_n = 90\n",
    "o_n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Represent output layer as a vector of 0 and ones where the position of 1 corresponds to the number we want to predict\n",
    "targets = np.array(np.zeros(o_n), ndmin=2).T\n",
    "targets[int(observation[0])] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a look at how our output layer looks like \n",
    "# (remember that our observations corrsponds to the number 5)\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have new structure of the NN we should reasign the weights - now the size of each weight matrix will increase as we have more nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the weights\n",
    "w_i_h = np.random.rand(h_n, i_n)\n",
    "w_h_o = np.random.rand(o_n, h_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 784), (784, 1), (10, 90), (10, 1))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sizes of matrices of weights, input and output vectors\n",
    "w_i_h.shape, inputs.shape, w_h_o.shape, targets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the structure of the NN updated for the specific task of prediciting numbers depicted on the images, we can  run our network to get its first predictions. To do so we will have to make several steps and each of them will consist of a matrix multiplication and application of the sygmoid function. Multiply a vector of inputs by a matrix of weights that connects it with the next layer, transform the result using activation function - this is one step and it schould be repeated for all the layers except for the input one. Every time the output of the previous leyer will be used as a vector of inputs for the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pics/multiplication.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"pics/activation.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the outputs of hidden and output layers of our NN\n",
    "h_inputs = np.dot(w_i_h, inputs)\n",
    "h_outputs = sigmoid(h_inputs)\n",
    "o_inputs = np.dot(w_h_o, h_outputs)\n",
    "o_outputs = sigmoid(o_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show intermediate data and outputs\n",
    "# h_inputs\n",
    "# h_outputs\n",
    "# o_inputs\n",
    "o_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like we have some serious mistake here. \n",
    "- Already at the point of h_outputs all the data converts to 1.\n",
    "- There could be several reasons for that. \n",
    "- First of all, let's take a look at our sigmoid function once again:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why don't we get what we expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the reason let's take a look at our sygmoid function once again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111be6dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the uotput of it will be almost identical once we feed a number bigger than 2 or smaller than -2. Big numbers are a problem for our activation fucntion as it can't find the difference between them. One solution will be to transform the inputs we have. They schould be in the range between 0 and 1. 0 schould not be included because the result of multiplication of an input equal to 0 by whichever weight will be equal to 0, hence our NN will not be able to use this input to learn anything valuable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The correct way to define inputs:\n",
    "inputs = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "# Correct way to define targets:\n",
    "targets = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "targets[int(observation[0])] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we can check our way to randomly assign initial weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04185732,  0.29350618,  0.63001588,  0.0126964 ],\n",
       "       [ 0.0646726 ,  0.06597453,  0.40517252,  0.66042894],\n",
       "       [ 0.95086174,  0.44122473,  0.45078402,  0.99430976]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the weights are positive, while there should be an apportunity for negative ways.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.33291307,  0.42110442, -0.6618691 ,  0.60917865],\n",
       "       [-0.82005401,  0.4071739 , -0.63484775, -0.72903569],\n",
       "       [ 0.06120286,  0.60725441,  0.30808221, -0.29828123]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The correct way to define random weights:\n",
    "    \n",
    "alternative1 = np.random.rand(3, 4) - 0.5 \n",
    "# or\n",
    "alternative2 = np.random.normal(0.0, pow(3, -0.5), (3, 4)) \n",
    "# second approach is better as it takes in account the standard deviation \n",
    "# that is related to the number of incoming links into a node, 1/√(number of incoming links).\n",
    "\n",
    "# alternative1\n",
    "alternative2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the weights in a proper way\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.21958917],\n",
       "       [ 0.10071162],\n",
       "       [ 0.578832  ],\n",
       "       [ 0.72442982],\n",
       "       [ 0.27875378],\n",
       "       [ 0.36385897],\n",
       "       [ 0.42552136],\n",
       "       [ 0.81739875],\n",
       "       [ 0.12968912],\n",
       "       [ 0.14382971]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run NN to get new prediciton for a particular observation\n",
    "h_inputs = np.dot(w_i_h, inputs)\n",
    "h_outputs = sigmoid(h_inputs)\n",
    "o_inputs = np.dot(w_h_o, h_outputs)\n",
    "o_outputs = sigmoid(o_inputs)\n",
    "o_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good are we now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all steps done in a right way we can check our output once again and compare it to target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20958917],\n",
       "       [-0.09071162],\n",
       "       [-0.568832  ],\n",
       "       [-0.71442982],\n",
       "       [-0.26875378],\n",
       "       [ 0.62614103],\n",
       "       [-0.41552136],\n",
       "       [-0.80739875],\n",
       "       [-0.11968912],\n",
       "       [-0.13382971]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_errors = targets - o_outputs\n",
    "o_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not expected any good result at this stage as all the weights have been assigned randomly and no training has been performed yet. Nevertheless, is not just useless vector of \"1\" anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of each node is the sum of the multiplications of the outputs of previous nodes by certain weights. Therefore we can associate how much error is comming with every weight and how much error has been brought from each particular node from the previous layer as depicted on the pictures below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp_2.png\"  alt=\"Drawing\" style=\"width: 900px;\"/> [Source: https://ebook4expert.com/2016/07/12/make-your-own-neural-network-ebook-free-by-tariq-rashid-epubmobi/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so in python we just need to make matrix miltiplication of erros and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the errors associated with hidden layer output\n",
    "h_errors = np.dot(w_h_o.T, o_errors)\n",
    "h_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Updating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, how do we improve the weights we have assigned randomly at the beginning, so that the overall resilt improves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula2.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula3.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula5.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# update the matrix for weights between hidden and output layers\n",
    "w_h_o += np.dot((o_errors * o_outputs * (1.0 - o_outputs)), np.transpose(h_outputs))\n",
    "# update the matrix for weights between input and hidden layers\n",
    "w_i_h += np.dot((h_errors * h_outputs * (1.0 - h_outputs)), np.transpose(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">via GIPHY</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the previous code does not display the gif file delete the previous cell and uncomment the following code\n",
    "# import IPython\n",
    "# url = 'https://giphy.com/embed/8tvzvXhB3wcmI'\n",
    "# iframe = '<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>'\n",
    "# IPython.display.HTML(iframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, there is something else, we forgot when updating our weights. If we completely change our weights with every new observation - our model learns to predict only the last input. Instead of updating weights 100 % every time we can change them only partially - this way every new observstion will bring some new knowledge while the previous one will still be in memory even though updated to certain extent. The bigger the learning rate the more importance has the last observation, the smaller it is the more important is all the previous knowledge. The smaller the steps - the more accurate will be the prediction. At the same time it might take more time to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/learning_rate.png\" alt=\"Drawing\" style=\"width: 600px;\"/> [Source: \"Business Analytics & Data Science Course by Professor S. Lessman, Chapter 5:\n",
    "Artificial Neural Networks\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the learning rate\n",
    "l_r = 0.3\n",
    "\n",
    "# update the weights for the links between the hidden and output layers\n",
    "w_h_o += l_r * np.dot((o_errors * o_outputs * (1.0 - o_outputs)), np.transpose(h_outputs))\n",
    "# update the weights for the links between the input and hidden layers\n",
    "w_i_h += l_r * np.dot((h_errors * h_outputs * (1.0 - h_outputs)), np.transpose(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put it in a bigger scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put all the steps done before in a loop, so that we can perform them not just for one observation\n",
    "but for all observations in our training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    observation = i.split(',')\n",
    "    inputs = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "    targets = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "    targets[int(observation[0])] = 0.99\n",
    "\n",
    "    h_inputs = np.dot(w_i_h, inputs)\n",
    "    h_outputs = sigmoid(h_inputs)\n",
    "    o_inputs = np.dot(w_h_o, h_outputs)\n",
    "    o_outputs = sigmoid(o_inputs)\n",
    "\n",
    "    o_errors = targets - o_outputs\n",
    "    h_errors = np.dot(w_h_o.T, o_errors)\n",
    "    \n",
    "    w_h_o += l_r * np.dot((o_errors * o_outputs * (1.0 - o_outputs)), np.transpose(h_outputs))\n",
    "    w_i_h += l_r * np.dot((h_errors * h_outputs * (1.0 - h_outputs)), np.transpose(inputs))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the model with all 100 observations we can test it with new data it has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the mnist test data CSV file\n",
    "raw_data_test = open(\"data/mnist_test.csv\", 'r')\n",
    "data_test = raw_data_test.readlines()\n",
    "raw_data_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check a particular observation\n",
    "observation = data_test[0].split(',')\n",
    "# print the label\n",
    "print(observation[0])\n",
    "# image the number\n",
    "image = np.asfarray(observation[1:]).reshape((28,28))\n",
    "mpp.imshow(image, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use this observation as an input and run NN with it\n",
    "inputs = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "h_inputs = np.dot(w_i_h, inputs)\n",
    "h_outputs = sigmoid(h_inputs)\n",
    "o_inputs = np.dot(w_h_o, h_outputs)\n",
    "o_outputs = sigmoid(o_inputs)\n",
    "\n",
    "o_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction of NN for this test observation\n",
    "label = np.argmax(o_outputs)\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test the neural network using all test dataset\n",
    "\n",
    "# scorecard of the network\n",
    "scorecard = []\n",
    "\n",
    "# go through all the observations in the test data set\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    inputs = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_inputs = np.dot(w_i_h, inputs)\n",
    "    h_outputs = sigmoid(h_inputs)\n",
    "    o_inputs = np.dot(w_h_o, h_outputs)\n",
    "    o_outputs = sigmoid(o_inputs)\n",
    "\n",
    "    label = np.argmax(o_outputs)\n",
    "\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is several times better than naive. Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Improvements?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# What if we train it more? What does this mean? Introduce local minimum concept.\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The \"big loop\" with epochs\n",
    "for e in range(epochs):\n",
    "    for i in data:\n",
    "        observation = i.split(',')\n",
    "        inputs = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "        targets = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "        targets[int(observation[0])] = 0.99\n",
    "\n",
    "        h_inputs = np.dot(w_i_h, inputs)\n",
    "        h_outputs = sigmoid(h_inputs)\n",
    "        o_inputs = np.dot(w_h_o, h_outputs)\n",
    "        o_outputs = sigmoid(o_inputs)\n",
    "\n",
    "        o_errors = targets - o_outputs\n",
    "        h_errors = np.dot(w_h_o.T, o_errors)\n",
    "        w_h_o += l_r * np.dot((o_errors * o_outputs * (1.0 - o_outputs)), np.transpose(h_outputs))\n",
    "        w_i_h += l_r * np.dot((h_errors * h_outputs * (1.0 - h_outputs)), np.transpose(inputs))\n",
    "\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "# test\n",
    "scorecard = []\n",
    "\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    inputs = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_inputs = np.dot(w_i_h, inputs)\n",
    "    h_outputs = sigmoid(h_inputs)\n",
    "    o_inputs = np.dot(w_h_o, h_outputs)\n",
    "    o_outputs = sigmoid(o_inputs)\n",
    "\n",
    "    label = np.argmax(o_outputs)\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        scorecard.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# calculate accuracy\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() /\n",
    "scorecard_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other l_r?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_r = 0.1\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More hidden nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_n = 150\n",
    "\n",
    "# Determine the weights for a bigger matrices\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It is much easier to train neural networks where the number of neurons is larger than required. But, with a smaller number of neurons the neural network has much better generalization abilities. It means it will respond correctly for patterns not used for training. If too many neurons are used, then the network can be overtrained on the training patterns, but it will fail on patterns never used in training. With a smaller number of neurons, the network cannot be trained to very small errors, but it may produce much better approximations for new patterns. The most common mistake made by many researchers is that in order to speed up the training process and to reduce the training errors, they use neural networks with a larger number of neurons than required. Such networks would perform very poorly for new patterns not used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = open(\"data/mnist_train.csv\", 'r')\n",
    "data = raw_data.readlines()\n",
    "raw_data.close()\n",
    "\n",
    "# Settings\n",
    "epochs = 2\n",
    "l_r = 0.1\n",
    "h_n = 90\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
