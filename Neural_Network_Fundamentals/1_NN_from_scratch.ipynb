{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Neural Network Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blog post is a guide to help readers build a neural network from the very basics. It starts with an introduction to the concept of a neural networks concept and its early development. A step-by-step coding tutorial follows, through which relevant concepts are illustrated. Later in the post, there is also an introduction on how to build neural networks in Keras. Finally, the reader will find instructions on how to deploy the model via an API to make it accessible to anyone interested on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks and its early development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the name tells, the idea of neural networks is inspired by how neurons work in the human brain. It is, however, crucial for the readers to know that despite the original motivation of neural networks, the NN models being used today have little resemblance to what a human brain does ???(Warner and Misra, 1996).  In its basic form, neural networks are composed of nodes interconnected to each other in several layers. The basic form of a NN would include an input, a hidden and an output layer. The number of nodes and layers can add to the complexity and efficiency of neural networks.  \n",
    "\n",
    "The McCulloch-Pitts model of neuron in 1943 was one of the earliest simplified version of neural networks. It consisted of a simple neuron which received a weighted sum of inputs and output either zero if the sum was smaller than a threshold or one when it was greater than the threshold. This idea is called firing and is an interesting analogy to what an actual neuron does. Later on, in the early 1960s, Rosenblatt introduced the simple perceptron model. This was a developed version of the McCulloch-Pitts with an input and output layer. However, the linear separablilty limitation ???(Minsky and Papert ,1969) of simple perceptron took away the research interest in neural networks for a while. In the early 1980s, the Hopfield model of content-addressable memory, however, motivated researchers in the area again and later on with the introduction of backpropagation learning algorithm, interest in neural networks research soared. Nowadays, neural nets are used in a variety of applications to tackle problems such as classification, speech and image recognition, control systems and predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mention:\n",
    "    * http://neuralnetworksanddeeplearning.com/index.html                  Michael Nielsen / Dec 2017\n",
    "    * http://www.cristiandima.com/neural-networks-from-scratch-in-python/  CRISTIAN DIMA\n",
    "    * [Rashid, 2016](https://ebook4expert.com/2016/07/12/make-your-own-neural-network-ebook-free-by-tariq-rashid-epubmobi/)]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# NN from scratch\n",
    "## Problem Statement\n",
    "The best way to understand how neural networks work is to build one yourself from scratch.\n",
    "The understanding becomes even more comprehensive if there is a particular problem that can be solved using NNs. Therefore let's start our work by taking a look at the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/problem.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are handwritten numbers that you want computer to correctly classify. This would be an easy task for a person but at least for a long period of time was an extremely complicated one for a machine. \n",
    "\n",
    "Even though the computer is faster than the human brain in numeric computations, the brain outperforms the computer in some other tasks. Many of those tasks are related to the human ability for sentience (which is a concept different from intelligence). The trick is to find a way, so that the computer could apply its numeric computation skills to solve these later tasks (at least to some degree).\n",
    "\n",
    "The first step would be to limit the scope of the task. In our particular case the broad task of image recognition will be addressed as a classification problem - a task of giving an object a label from a given set of labels.\n",
    "\n",
    "As we will see during the process of building our own NN, its output is based almost exclusively on application of linear algebra methods. Despite the name (which is sometimes related to the fear of artificial intelligence), neural networks in fact are much more related to statistical methods (like regression analysis or curve fitting) than to the way human brain works [[Stuart Reid, 2014](http://www.turingfinance.com/misconceptions-about-neural-networks/)]. \n",
    "\n",
    "NNs are inspired by human brain only to certain extent. For instance the main element that makes them similar is a multilayer net structure of simple elements that are connected in some way, receiving and transmitting information. But the structure of the human brain is much more complicated, besides it is self-organizing and adaptive in contrast to the fixed manually designed architecture of a NN. Hence, there is a good reason to stop being afraid of neural networks and instead to create one ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neurons_net3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> [Source: [Pixabay.com](https://pixabay.com/)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schematic Representation\n",
    "\n",
    "A complex multilayer structure that all neural networks have in common in a simplified way can be depicted using the following picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need in order to implement such a structure is base Python and numpy, a library for numerical computation, which we will be using to do linear algebra. \n",
    "\n",
    "First let's determine the elements of a neural network depicted above: nodes, layers, weights across nodes and activation functions.\n",
    "\n",
    "**Nodes.** A node is basically a point where data points are received, processed and then transferred to the node. A node could be either an endpoint or a redistribution point or even both when iterations are done through the learning algorithm. The number of nodes to use is optional.\n",
    "\n",
    "**Layers.** A layer consists of one or several nodes. The initial layer in the network is called the input layer and it is the entry point through which the data is fed into the neural net. The middle layers are called hidden layer because the computation results of them are not directly visible to someone interacting with the neural net. In the hidden layers, which can range from one to thousands, the features are transformed and most of the structure (both linear and nonlinear) is captured. Finally, there is the final layer, from which results are output. The nodes in each layer are fully interconnected to the ones in the next and the previous layers. \n",
    "\n",
    "In our case we have a structure with 3 layers: input, output and one hidden layer. The number of nodes in the input  (\"i_n\"), hidden (\"h_n\") and output (\"o_n\") layers are 3, 5 and 2 respectively. In Python, such a structure can be represented in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to work with numbers:\n",
    "import numpy as np\n",
    "\n",
    "# Determine the structure of the NN:\n",
    "i_n = 3\n",
    "h_n = 5\n",
    "o_n = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights.** In order to transfer an input data point to the next layer, a predetermined number (called weight) is stored in each connection from the sender node to the receiver node. Each weight accounts for the impact between the interconnected nodes.\n",
    "\n",
    "Initially, we assign weights between nodes in neighboring layers randomly. This is needed only for the sake of initializing the structure. Later these weights will be changed in order to solve our classification problem. The weight updating will be better described in the following sections.\n",
    "\n",
    "Neural nets will have n-1 matrices of weights, where n is the number of layers in the NN. You can imagine these weight matrices sitting between two layers representing the strength of the connection between every single node of neighbouring layers. Thus, each of these matrices will be of size f x p, where p is the number of nodes in the preceding layer and f is the number of nodes in the following layer.\n",
    "\n",
    "This becomes more clear once you check the code below that creates 2 matrices of weights:\n",
    "* matrix of weights between input and hidden layers (\"w_i_h\") - 5 by 3 matrix\n",
    "* matrix of weights between hidden and output layers (\"w_h_o\") - 2 by 5 matrix.\n",
    "\n",
    "Such a dimensions of matrices are necessary in order to accomplish matrix and vector multiplications that are done in the following stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.90777842,  0.73311418,  0.11434952],\n",
       "       [ 0.98934683,  0.76666849,  0.69625713],\n",
       "       [ 0.30095531,  0.72558061,  0.50910764],\n",
       "       [ 0.96351366,  0.65874791,  0.06047322],\n",
       "       [ 0.94092134,  0.3455386 ,  0.83219342]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly define the weights between the layers:\n",
    "w_i_h = np.random.rand(h_n, i_n) # create an array of the given shape and populate it with random values.\n",
    "w_h_o = np.random.rand(o_n, h_n) \n",
    "\n",
    "# Show matrices of randomly assigned weights:\n",
    "w_i_h\n",
    "# w_h_o # uncomment this line in order to see the values for w_h_o.\n",
    "# Use Cmd + / in MacOS and CTRL + / in MS Windows as a shortcut to comment/uncomment lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activation Function.** The remaining element of the NN's structure is an activation function - a function which transforms an input data point that it receives from the previous nodes to an output value which will be the input for the nodes in the next layer. The activation function plays an important role in the efficiency of the neural network as it accounts for non-linearity of data. \n",
    "It is to certain extent inspired by the concept of \"firing\", which means that neurons \"fire\" or transmit information further only if the input surpasses certain threshold. The simplest activation function can be represented by a step function as on the picture below!!!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/step_function.png\" alt=\"Drawing\" style=\"width: 700px\";/> [Source: [Research Gate](https://www.researchgate.net/figure/Three-different-types-of-transfer-function-step-sigmoid-and-linear-in-unipolar-and_306323136)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our NN, we will use a slightly more elaborate activation function, the sigmoid function (logistic), which allows for more efficient use of the input data. Extended description of various activation functions, their benefits and disadvantages is given in sections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine activation function:\n",
    "def sigmoid(x):\n",
    "    # np.exp() calculates the exponential\n",
    "    # of all elements in the input array.\n",
    "    return 1 / (1 + np.exp(-x)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f21df98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Draw activation function:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# return 100 evenly spaced numbers over an interval from -10 to 10.\n",
    "x = np.linspace(-10, 10, 100) \n",
    "# plot sigmoid function for sampled values:\n",
    "plt.plot(x, sigmoid(x)) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection\n",
    "\n",
    "By now we have collected all the elements of the NN. Can we use this structure in order to solve the classification problem stated in the beginning? In order to answer this question we need first to get a better understanding of the data at our disposal. \n",
    "\n",
    "We are trying to check whether NN is able to solve the classification problem using a collection of 70 000 handwritten numbers. Each of this handwritten number is represented as 28x28 image. \n",
    "\n",
    "The original source of the data is \"THE MNIST DATABASE\". A detailed description of the dataset is available at\n",
    "http://yann.lecun.com/exdb/mnist/. There you can also find, for example, a summary of the performance results achieved by various classification algorithms.\n",
    "\n",
    "For the sake of simplicity we suggest obtaining the data from another source:\n",
    "https://pjreddie.com/projects/mnist-in-csv/. Here the original images are saved in CSV, which allows to work with them directly.\n",
    "\n",
    "For the purposes of demonstration below we use a smaller dataset (100 images), which will be expanded at a later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data:\n",
    "raw_data = open(\"data/mnist_train_100.csv\", 'r') # \"r\" stands for \"read only\" mode.\n",
    "data = raw_data.readlines() # read all the lines of a file in a list.\n",
    "raw_data.close() # remove temporal file from the environment in order to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the data - check the number of observations:\n",
    "len(data) # length of the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,10,254,254,254,254,255,209,126,67,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,119,230,233,241,216,248,254,254,239,223,132,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,21,25,38,0,48,126,245,244,251,254,243,198,131,11,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,66,113,192,254,254,235,25,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,29,226,254,37,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,107,254,249,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,26,186,254,254,130,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,96,216,254,228,100,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,11,89,212,254,254,210,38,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,127,213,255,254,247,66,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,57,254,219,71,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,157,254,202,78,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,7,91,251,231,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,98,254,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,30,238,244,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,19,19,4,0,0,0,0,5,228,244,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,142,254,123,14,0,0,0,0,129,254,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,219,254,40,0,0,34,103,221,254,210,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,72,254,233,139,57,232,254,243,166,28,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,94,212,254,254,141,101,41,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect a particular observation of the data:\n",
    "data[0] # show observation number 0 from the list (remember that in Python numbering starts from 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A particular observation looks like a string of 785 elements (label of the image + 784 elements for each pixels of a 28x28 image). \n",
    "* Each element representing a pixel is a number from 0 to 255 (from white to black color).\n",
    "* The first element in the line is the label of the image and therefore is a number from 0 to 9.\n",
    "\n",
    "Using `matplotlib`, we can also reconstruct the original image based on the data about each pixel in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the package to plot the data:\n",
    "import matplotlib.pyplot as mpp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f404d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQFD/D0PAA9CKNoDjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f322b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data:\n",
    "observation = data[0].split(',') # break down observation number 0 (comma is used to identify each element).\n",
    "image = np.asfarray(observation[1:]).reshape((28,28)) # take all the elements starting from the element 1 \n",
    "# (exclude element number 0, that corresponds to the label) and reshape them as an array with dimension 28 by 28.\n",
    "mpp.imshow(image, cmap='Blues', interpolation='None') # show the plot of this array using blue pallete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save an observation of the data as an input to work with:\n",
    "input = np.array(np.asfarray(observation[1:]), ndmin=2).T # save necessary elements in a vertical vector shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the structure of the NN to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once again at the NN's structure we have created at the beginning of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/neural_network1.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the data, we can conclude that the structure with 3-5-2 nodes is probably not optimal and therefore should be updated in order to fit the data we have and peculiarities of the classification problem: \n",
    "\n",
    "* For each observation we have 784 elements as an input (label element is excluded). Accordingly, instead of 3 input nodes we should better have 784. \n",
    "* Similarly, as we have 10 different options for the outcome (handwritten numbers are labeled from 0 to 9) the number of output nodes should be 10 instead of 2. \n",
    "* We also change the number of hidden nodes from 5 to 90. Such a number has been assigned based on some proportionality assumptions which will be checked later: 90 is 9 times higher than 10 and approximately 9 times smaller than 784."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the new structure of the NN:\n",
    "i_n = 784\n",
    "h_n = 90\n",
    "o_n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have new structure of the NN we should reassign the weights - now the size of each weight matrix will increase as we have more nodes in each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine the weights:\n",
    "w_i_h = np.random.rand(h_n, i_n)\n",
    "w_h_o = np.random.rand(o_n, h_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have not used the first element of our observation - the label. It will be necessary to compare the predictions of the NN to the real state of the world and to train the NN to make correct predictions. The target should therefore have the same shape as the output layer of the NN, so that they could be comparable. We can represent the label as a vector of n binary (0 or 1) elements (n corresponds to the number of nodes in the output layer). There should be only one element equal to 1 and the position of this element should correspond to the index number of the label we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create target array:\n",
    "target = np.array(np.zeros(o_n), ndmin=2).T\n",
    "target[int(observation[0])] = 1 # int() method returns an integer object from any number or string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect how the target looks like (remember that the label of observations is 5):\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90, 784), (784, 1), (10, 90), (10, 1))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the sizes of matrices of weights, input and target vectors:\n",
    "w_i_h.shape, input.shape, w_h_o.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforwarding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the structure of the NN updated for the specific task of classifying the numbers depicted on the images, we can run our network in order to get the first predictions that will be represented by a vector of 10 elements. This vector in its turn can be compared to the target.\n",
    "\n",
    "To run the NN, i.e. to feed forward our input data in order to get some predictions, we should follow certain steps:\n",
    "\n",
    "1. Multiply an input vector by a matrix of weights that connects it with the next layer;\n",
    "2. Transform the result using activation function;\n",
    "3. Use the output obtained in the 2nd step as an input vector for the next layer.\n",
    "\n",
    "A sequence of this steps should be repeated n-1 times (where n corresponds to the number of layers). The output of the previous layer will always be the input vector for the next layer. In our case the procedure will happen twice.\n",
    "\n",
    "In the picture bellow !!!, you can see the procedure necessary to obtain the output of the hidden layer. The result of matrix multiplication here is called \"Hidden_Input\". Result of the transformation of \"Hidden_Input\" through activation function is called \"Hidden_Output\".\n",
    "\n",
    "This output will be used as the input vector that should be multiplied by the next weight matrix and transformed through activation function in order to calculate the final output of the NN. If our NN would have more than one hidden layer, the procedure would be repeated more times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/multiplication.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/activation.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can see the code implementation of all the steps for all layers of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the output of hidden and output layers of our NN:\n",
    "h_input = np.dot(w_i_h, input) # dot() performs matrix multiplication; \"h_input\" stands for \"Hidden_Input\".\n",
    "h_output = sigmoid(h_input) # \"Hidden_Output\" - result after activation function.\n",
    "o_input = np.dot(w_h_o, h_output) # \"Output_Input\" - input used for the next layer.\n",
    "o_output = sigmoid(o_input) # \"Output_Output\" - final output of the NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.],\n",
       "       [ 1.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show intermediate data and output:\n",
    "# Uncomment the line of interest in order to see the the corresponding object.\n",
    "# h_input\n",
    "# h_output\n",
    "# o_input\n",
    "o_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data treatment good practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we check the output of the NN and the results of each performed step, we can observe that already at the stage of the h_output all the data converts to a vector in which all the values are equal to 1. Such a vector does not provide us with any helpful insight. Apparently, something is wrong with what we have done so far. There could be several reasons for the problem we face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First of all, let's take a look at our sigmoid function once again:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JjS7gxYOYDAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f3f2a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-10, 10, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the output of the sigmoid function will be almost identical once we feed a number bigger than 2. Similarly there is no significant difference between the outputs if numbers used are smaller than -2. Hence the application of sigmoid function to the original data leads to a loss of valuable information. The NN has problems learning something from the inputs, which are almost undifferentiable.  \n",
    "\n",
    "One solution is to transform the input we have. Ideally we should have our data in a range between 0 and 1. It is also desirable to avoid zeros as inputs, because the output of a zero input will always be zero, no matter how large the weights are, in which case the NN will not be able to use this input to learn.\n",
    "\n",
    "We can perform a transformation of the original data as the one coded below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Good practice transformation of the input values:\n",
    "input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T \n",
    "# Our values in our input vector are in the range from 0 to 255. Therefore we should divide input vector by 255, \n",
    "# multiply it by 0,99 and add 0,01 in order to get values in the range from 0,01 to 1.\n",
    "\n",
    "# Good practice transformation of the target value:\n",
    "target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "target[int(observation[0])] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Secondly, we can check our way to randomly assign initial weights:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look once at the function we used to randomly assign weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22583725,  0.12109248,  0.02590624,  0.12690234,  0.62018663],\n",
       "       [ 0.81179976,  0.46311266,  0.71892529,  0.44099911,  0.43754171],\n",
       "       [ 0.49648974,  0.30002541,  0.04783774,  0.69393036,  0.58613769]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all the weights are positive, while the actual relationship between the features in the data and the values of the output vector can be negative. Hence, the way we employ to assign random weights should allow for negative weights too.\n",
    "\n",
    "Below there are too alternatives how this can be implemented in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06685916, -0.17391294,  0.14547523, -0.48137787,  0.03086745],\n",
       "       [-0.06936764,  0.21306989, -0.89904039, -0.99947961,  0.22077087],\n",
       "       [ 0.85971567, -0.21178824, -0.26656818, -0.30837416,  0.92054582]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good practice for initial weights assignment:\n",
    "    \n",
    "alternative1 = np.random.rand(3, 5) - 0.5 \n",
    "# or\n",
    "alternative2 = np.random.normal(0.0, pow(3, -0.5), (3, 5)) \n",
    "# arguments: Mean of the distribution, Standard deviation of the distribution, Output shape.\n",
    "# Second approach is better as it takes in account the standard deviation \n",
    "# that is related to the number of incoming links into a node, 1/√(number of incoming links).\n",
    "\n",
    "# alternative1\n",
    "alternative2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the weights in accordance with the best practice:\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the elements assigned in accordance with good practices, we can feedforward the data once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07373055],\n",
       "       [ 0.14887152],\n",
       "       [ 0.96694188],\n",
       "       [ 0.70200766],\n",
       "       [ 0.24295729],\n",
       "       [ 0.22478192],\n",
       "       [ 0.75462671],\n",
       "       [ 0.66361842],\n",
       "       [ 0.37623992],\n",
       "       [ 0.47408493]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run NN to get new classification of the particular observation:\n",
    "h_input = np.dot(w_i_h, input)\n",
    "h_output = sigmoid(h_input)\n",
    "o_input = np.dot(w_h_o, h_output)\n",
    "o_output = sigmoid(o_input)\n",
    "o_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First evaluation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have obtained the output of the NN, we can compare it to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.06373055],\n",
       "       [-0.13887152],\n",
       "       [-0.95694188],\n",
       "       [ 0.28799234],\n",
       "       [-0.23295729],\n",
       "       [-0.21478192],\n",
       "       [-0.74462671],\n",
       "       [-0.65361842],\n",
       "       [-0.36623992],\n",
       "       [-0.46408493]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the errors of the classification:\n",
    "o_errors = target - o_output\n",
    "o_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we would like to achieve should look like as a vector of values where almost all values are negligibly small except for the one value that has the position in the vector corresponding to the index of the true label. \n",
    "\n",
    "It is not the case now. Nevertheless one should remember that so far all the weights have been assigned randomly and no training has been performed yet. In any case, it is not a vector of ones anymore, which an improvement.\n",
    "\n",
    "Thus, we can proceed to the next stage, which is to find out where the errors come from and how they can be minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back propagation is a learning algorithm which aims to minimize the errors/cost function of the NN. Through this learning algorithm, the random weights and biases which were initially given to the network will be optimized to give the best output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output of each node is the sum of the multiplications of the output of previous nodes by certain weights. Therefore we can associate how much error is coming with every weight and how much error has been brought from each particular node from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this better it is worth imagining the following example:\n",
    "* node 1 in the output layer of the NN should be equal to 0,01 ;\n",
    "* instead the NN is providing us with 0,8.\n",
    "\n",
    "In this case we should do the following:\n",
    "\n",
    "1. Calculate the error of the node (-0,79 in our example);\n",
    "\n",
    "2. Calculate how much error has been brought by every link to this node.\n",
    "\n",
    "For instance if weight w<sub>11</sub> is 0,6 and w<sub>21</sub> is 0,4 then they are associated with an error of -0,79 multiplied by 0,6 and -0,79 multiplied by 0,4 respectively (see Pictures below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/bp.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculation of how much error is associated with every weight we can obtain the errors for the nodes in the proceeding layer.\n",
    "\n",
    "For instance error term for node 1 in the hidden layer will we equal to:\n",
    "\n",
    "the sum of errors associated with all the weights (w<sub>11</sub> and w<sub>12</sub> in our case) that link this node with the next layer. (see Picture below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we repeat this procedure for all the nodes in all layers we can find out how much every node should be changed.\n",
    "\n",
    "To do so in Python we just need to make multiplication of vector that contain errors by corresponding matrix of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.39443768],\n",
       "       [-0.16865836],\n",
       "       [ 0.0304721 ],\n",
       "       [-0.85442941],\n",
       "       [-0.19828127],\n",
       "       [-0.53651297],\n",
       "       [ 0.52033741],\n",
       "       [-0.2781908 ],\n",
       "       [-0.07071894],\n",
       "       [-1.63579796]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the errors associated with hidden layer output:\n",
    "h_errors = np.dot(w_h_o.T, o_errors)\n",
    "h_errors[0:10] # errors in the hidden layer - show the first 10 nodes out of 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is one the most popular algorithms to optimize the neural networks. The name gradient descent is rooted in the procedure where the gradient is repeatedly evaluated to update the parameters. The objective of the gradient descent is to find weight parameters that will minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the concept of gradient descent we should ask ourselves the following question: What can be done to improve the weights we have assigned randomly at the beginning, so that the overall result improves? \n",
    "\n",
    "To change the output of any node we should change the weights that connect it with the previous layer. Basically we want to find out how much error in every node changes once we change associated weights. Next we want to select the weights that would lead to a minimal error in the output layer. That can be achieved by differentiation of the cost function and search for its minimum.\n",
    "\n",
    "Given multidimensionality of the function, which we need to differentiate, the search for its minimum can be a complicated task. This task is similar to some extent to the search of the path in the darkness from the top of a mountain to its valley. Because it is dark it is almost impossible to reach the valley immediately. The only way to achieve the goal is by exploring the neighbourhood (the radius you are able to see) and tacking small steps in the direction that leads downhill and constantly updating the path for the next steps. This process is illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
       "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">[Source: Giphy.com]</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe src=\"https://giphy.com/embed/8tvzvXhB3wcmI\" width=\"1000\" height=\"400\" frameBorder=\"0\" class=\"giphy-embed\" allowFullScreen></iframe>\n",
    "<p><a href=\"https://giphy.com/gifs/deep-learning-8tvzvXhB3wcmI\">[Source: Giphy.com]</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically the differentiation process can be illustrated on the example of weights between output and hidden layers (Who). The same process but with corresponding values should be applied for the weights between input and hidden layers (Wih).\n",
    "\n",
    "As it can be seen from the formulas !!! the error we want to minimize (E) can be defined as the sum of squared differences between the target (Tn) and output (On) values of the NN. The sum of differences for all the nodes in the layer is relevant but when doing calculation for a particular node this sum can be omitted - only the difference between particular output (Oo) and target (Oo) matters.\n",
    "\n",
    "Target value is constant. Output value depends on weights and is obtained after applying sigmoid function to the sum of inputs (outputs of the previous layer - Oh) multiplied by corresponding weights (Who)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula2.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for derivative of the sigmoid function is provided below !!! It is necessary to keep in mind that the sum to which we apply sigmoid function also depends on the change of weights (Who). Therefore one should follow the chain rule for derivation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula3.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula we derive is for one particular node. We can however apply it to all the nodes in the layer. In order to do so the only thing we need is to consider this formula in matrix notation. Thus, necessary update of weights linked to all the nodes in a layer will be calculated.\n",
    "\n",
    "After solving the minimization problem we can update the weights we have assigned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/formula5.png\"  alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In code this can be represented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the matrix for weights between hidden and output layers:\n",
    "w_h_o += np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "# Update the matrix for weights between input and hidden layers:\n",
    "w_i_h += np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there is something else, we should add in the weights updating procedure. If we completely change our weights with every new observation - our model learns to predict only the last input. Instead of updating weights 100 % every time we can change them only partially - this way every new observation will bring some new knowledge while the previous one will still be in memory even though updated to certain extent. The bigger the learning rate the more importance has the last observation, the smaller it is the more important is all the previous knowledge. The smaller the steps - the more accurate will be the prediction. At the same time it might take more time to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pics/learning_rate.png\" alt=\"Drawing\" style=\"width: 600px;\"/> [Source: \"Business Analytics & Data Science Course by Professor S. Lessmann, Chapter 5:\n",
    "Artificial Neural Networks\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code for weight's update procedure with learning rate included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the learning rate:\n",
    "l_r = 0.3\n",
    "\n",
    "# Update the weights for the links between the hidden and output layers:\n",
    "w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "# Update the weights for the links between the input and hidden layers:\n",
    "w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have been working with one particular observation. Let's put all the steps done before in a for-loop, so that we can perform them for all observations in our training set. More observations will allow the NN to learn from more information. Every time a new observation is feedforwarded, the error term backpropagated and the cost function minimized, the matrices of weights become more capable to label yet unknown observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    observation = i.split(',')\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "    target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "    target[int(observation[0])] = 0.99\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    o_errors = target - o_output\n",
    "    h_errors = np.dot(w_h_o.T, o_errors)\n",
    "    \n",
    "    w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "    w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second evaluation of the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have trained the model with 100 observations we can test it with new data it has never seen. After loading the test set we can first work with a particular observation to get an intuition about how good our NN can solve considered classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the mnist test data CSV file:\n",
    "raw_data_test = open(\"data/mnist_test.csv\", 'r')\n",
    "data_test = raw_data_test.readlines()\n",
    "raw_data_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10f518828>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAADYNJREFUeJzt3X2MXPV1xvHnqcGtwYBsdnEce23H\nxKIgp5iycloZpU4pkUmpDKmCsNLUlZI4bYPaVKkKcquAoqKgKAlBbUW1FAcjGRIqQnFUq0ANrZsI\nURbq+qVuiEXX4Hhl78YEY7UIsE//2OtoY3buDPN2Z32+H8mamXvuy9HAs3dmfjP354gQgHx+ruoG\nAFSD8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSOqsbh6sr68vFi9e0s1DAqkcODCi8fFxN7Ju\nS+G3vUbS3ZJmSPq7iLizbP3Fi5fo+88Ot3JIACVWfXCw4XWbftlve4akv5F0raTLJK2zfVmz+wPQ\nXa28518paX9EvBQRb0r6lqS17WkLQKe1Ev4Fkl6Z9Phgsexn2N5ge9j28Nj4WAuHA9BOrYR/qg8V\n3vH74IgYiojBiBjs7+tv4XAA2qmV8B+UNDDp8UJJh1prB0C3tBL+5yQts/0+2zMl3SRpa3vaAtBp\nTQ/1RcTbtm+W9Lgmhvo2RcTetnUGoKNaGuePiG2StrWpFwBdxNd7gaQIP5AU4QeSIvxAUoQfSIrw\nA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOEHkiL8QFKEH0iK\n8ANJEX4gKcIPJEX4gaQIP5AU4QeSIvxAUoQfSKqlWXptj0h6XdIJSW9HxGA7mgLQeS2Fv/DhiBhv\nw34AdBEv+4GkWg1/SHrC9vO2N7SjIQDd0erL/lURccj2RZKetP3fEbFj8grFH4UNkjSwaFGLhwPQ\nLi2d+SPiUHF7RNKjklZOsc5QRAxGxGB/X38rhwPQRk2H3/a5ts87dV/SRyTtaVdjADqrlZf98yQ9\navvUfh6MiH9qS1cAOq7p8EfES5Iub2MvALqIoT4gKcIPJEX4gaQIP5AU4QeSIvxAUu34VV8K/7h3\ntGbtzx/aVbrtggXnl9bP+fny/wx/cfWy0vrc2TNr1gYuPKd0W+TFmR9IivADSRF+ICnCDyRF+IGk\nCD+QFOEHkmKcv0G/86VttYv/s7N02wMtHvuf/7bOCuf11SxduDzvr64XDsypWbtn3RWl215a57sZ\nZwLO/EBShB9IivADSRF+ICnCDyRF+IGkCD+QFOP8DXr8yzfUrD098qHSbVeVjDdL0vdfebW0/tTe\nI6X1f3+q9vUEfvzM9tJttegD5fWXd5fXW3FW7esQSJL6l5TXR18sLf/4mdq1ry68oHTb+25aUX7s\nMwBnfiApwg8kRfiBpAg/kBThB5Ii/EBShB9Iqu44v+1Nkq6TdCQilhfL5kr6tqQlkkYk3RgR5YPV\n09zKpXObqjXiqmW1f48vSbf8evl1+499emXN2oujx0u3vfS955XW9x46VlpvxS+cNaO0PnDhrNL6\n0t++q/wARw/WLF05cOb/Xr+eRs7890tac9qyWyVtj4hlkrYXjwFMI3XDHxE7JB09bfFaSZuL+5sl\nXd/mvgB0WLPv+edFxKgkFbcXta8lAN3Q8Q/8bG+wPWx7eGx8rNOHA9CgZsN/2PZ8SSpua/7yJCKG\nImIwIgb7+/qbPByAdms2/FslrS/ur5f0WHvaAdAtdcNv+yFJz0i6xPZB25+SdKeka2z/UNI1xWMA\n00jdcf6IWFejdHWbe0GTzp91ds3a4NLyawnU0+p3GFqxbe9o+QqvHiotn/tLq2rWPnHFQDMtnVH4\nhh+QFOEHkiL8QFKEH0iK8ANJEX4gKS7djcocPf5maf0Tf/Zg+Q5OnigtD/3Jr9WsXXBO7eHRLDjz\nA0kRfiApwg8kRfiBpAg/kBThB5Ii/EBSjPOjMn+5fX/5CmMj5fU57y0tX9JXflny7DjzA0kRfiAp\nwg8kRfiBpAg/kBThB5Ii/EBSjPOjo3a9/FrN2je/8s2W9v300B+W1i+eN7ul/Z/pOPMDSRF+ICnC\nDyRF+IGkCD+QFOEHkiL8QFJ1x/ltb5J0naQjEbG8WHa7pM9IGitW2xgR2zrVJKavoeFXahffeqN0\n24Hf+M3S+vKB85tpCYVGzvz3S1ozxfK7ImJF8Y/gA9NM3fBHxA5JR7vQC4AuauU9/822d9neZHtO\n2zoC0BXNhv8eSRdLWiFpVNLXaq1oe4PtYdvDY+NjtVYD0GVNhT8iDkfEiYg4KeleSStL1h2KiMGI\nGOzv62+2TwBt1lT4bc+f9PAGSXva0w6AbmlkqO8hSasl9dk+KOk2Sattr5AUkkYkfbaDPQLogLrh\nj4h1Uyy+rwO9YBp6460TpfXvPrGvdnHmrNJtt/z+r5bWz5rBd9RawbMHJEX4gaQIP5AU4QeSIvxA\nUoQfSIpLd6MlX3z8xdL6sRd21Kwtvfa3Srf9wKILmuoJjeHMDyRF+IGkCD+QFOEHkiL8QFKEH0iK\n8ANJMc6PUv/yg/JLr917x73lOzj/opqlzZ/+YDMtoU048wNJEX4gKcIPJEX4gaQIP5AU4QeSIvxA\nUozzJ/fa/75VWv/Ybd8t38GJ8u2vvG51zdryAX6vXyXO/EBShB9IivADSRF+ICnCDyRF+IGkCD+Q\nVN1xftsDkh6Q9B5JJyUNRcTdtudK+rakJZJGJN0YEa92rlU048TJKK1f/qePldbjpf8orc94/5Wl\n9ft/d7C0juo0cuZ/W9IXIuJSSb8i6XO2L5N0q6TtEbFM0vbiMYBpom74I2I0Il4o7r8uaZ+kBZLW\nStpcrLZZ0vWdahJA+72r9/y2l0i6QtKzkuZFxKg08QdCUu3rNQHoOQ2H3/ZsSY9I+nxEHHsX222w\nPWx7eGy8/HpwALqnofDbPlsTwd8SEd8pFh+2Pb+oz5d0ZKptI2IoIgYjYrC/r78dPQNog7rht21J\n90naFxFfn1TaKml9cX+9pPKPjQH0lEZ+0rtK0icl7ba9s1i2UdKdkh62/SlJL0v6eGdaRCt+dPT/\nSuuvDf9rS/t/+IsfLa0vnDurpf2jc+qGPyK+J8k1yle3tx0A3cI3/ICkCD+QFOEHkiL8QFKEH0iK\n8ANJcenuM8DoT96oWbv8D7a0tO9bvvxHpfUPX8K3NqcrzvxAUoQfSIrwA0kRfiApwg8kRfiBpAg/\nkBTj/GeAO57aX7t4YFdL+177i/NK6xPXesF0xJkfSIrwA0kRfiApwg8kRfiBpAg/kBThB5JinH8a\n+M8DPymtb/nrv+9SJziTcOYHkiL8QFKEH0iK8ANJEX4gKcIPJEX4gaTqjvPbHpD0gKT3SDopaSgi\n7rZ9u6TPSBorVt0YEds61Whmj+w7XL7C8aNN73vG+68src+aOaPpfaO3NfIln7clfSEiXrB9nqTn\nbT9Z1O6KiK92rj0AnVI3/BExKmm0uP+67X2SFnS6MQCd9a7e89teIukKSc8Wi262vcv2Jttzamyz\nwfaw7eGx8bGpVgFQgYbDb3u2pEckfT4ijkm6R9LFklZo4pXB16baLiKGImIwIgb7+5jXDegVDYXf\n9tmaCP6WiPiOJEXE4Yg4EREnJd0raWXn2gTQbnXD74nLs94naV9EfH3S8vmTVrtB0p72twegUxr5\ntH+VpE9K2m17Z7Fso6R1tldICkkjkj7bkQ7Rktkrriqt7/nGx0rrF5xzdjvbQQ9p5NP+70ma6uLs\njOkD0xjf8AOSIvxAUoQfSIrwA0kRfiApwg8kxaW7p4EvrbmkTv2vutQJziSc+YGkCD+QFOEHkiL8\nQFKEH0iK8ANJEX4gKUdE9w5mj0k6MGlRn6TxrjXw7vRqb73al0RvzWpnb4sjoqHr5XU1/O84uD0c\nEYOVNVCiV3vr1b4kemtWVb3xsh9IivADSVUd/qGKj1+mV3vr1b4kemtWJb1V+p4fQHWqPvMDqEgl\n4be9xvYPbO+3fWsVPdRie8T2bts7bQ9X3Msm20ds75m0bK7tJ23/sLidcpq0inq73faPiudup+2P\nVtTbgO2nbe+zvdf2HxfLK33uSvqq5Hnr+st+2zMkvSjpGkkHJT0naV1E/FdXG6nB9oikwYiofEzY\n9ockHZf0QEQsL5Z9RdLRiLiz+MM5JyJu6ZHebpd0vOqZm4sJZeZPnlla0vWSfk8VPnclfd2oCp63\nKs78KyXtj4iXIuJNSd+StLaCPnpeROyQdPS0xWslbS7ub9bE/zxdV6O3nhARoxHxQnH/dUmnZpau\n9Lkr6asSVYR/gaRXJj0+qN6a8jskPWH7edsbqm5mCvOKadNPTZ9+UcX9nK7uzM3ddNrM0j3z3DUz\n43W7VRH+qWb/6aUhh1UR8cuSrpX0ueLlLRrT0MzN3TLFzNI9odkZr9utivAflDQw6fFCSYcq6GNK\nEXGouD0i6VH13uzDh09NklrcHqm4n5/qpZmbp5pZWj3w3PXSjNdVhP85Sctsv8/2TEk3SdpaQR/v\nYPvc4oMY2T5X0kfUe7MPb5W0vri/XtJjFfbyM3pl5uZaM0ur4ueu12a8ruRLPsVQxjckzZC0KSLu\n6HoTU7C9VBNne2niysYPVtmb7YckrdbEr74OS7pN0j9IeljSIkkvS/p4RHT9g7cava3WxEvXn87c\nfOo9dpd7u0rSv0naLelksXijJt5fV/bclfS1ThU8b3zDD0iKb/gBSRF+ICnCDyRF+IGkCD+QFOEH\nkiL8QFKEH0jq/wHJd6hLyIcL4wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f44f7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check a particular observation:\n",
    "observation = data_test[0].split(',')\n",
    "# Print the label:\n",
    "print(observation[0])\n",
    "# Image the number:\n",
    "image = np.asfarray(observation[1:]).reshape((28,28))\n",
    "mpp.imshow(image, cmap='Blues', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05086044],\n",
       "       [ 0.01692228],\n",
       "       [ 0.03306648],\n",
       "       [ 0.01855151],\n",
       "       [ 0.17733202],\n",
       "       [ 0.01942656],\n",
       "       [ 0.01083799],\n",
       "       [ 0.33279056],\n",
       "       [ 0.13214786],\n",
       "       [ 0.05988345]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this observation as an input and run NN with it:\n",
    "input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "h_input = np.dot(w_i_h, input)\n",
    "h_output = sigmoid(h_input)\n",
    "o_input = np.dot(w_h_o, h_output)\n",
    "o_output = sigmoid(o_input)\n",
    "\n",
    "o_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prediction of NN for this test observation:\n",
    "label = np.argmax(o_output)\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working with a particular observation from the testset we can label all of them and evaluate the accuracy of our NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the neural network using all test dataset:\n",
    "\n",
    "score = [] # create a list in which the predictions of the network will we saved.\n",
    "\n",
    "# Go through all the observations in the test data set:\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    expected = int(observation[0])\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    label = np.argmax(o_output)\n",
    "\n",
    "    if (label == expected):\n",
    "        score.append(1)\n",
    "    else:\n",
    "        score.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.3959\n"
     ]
    }
   ],
   "source": [
    "# Calculate the performance score, the fraction of correct answers:\n",
    "score_array = np.asarray(score)\n",
    "print (\"performance = \", score_array.sum() / score_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is several times better than naive, which would be 0.1 (given that we have 10 levels of the categorical variable we have to classify). Can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training with several epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve the results of the NN is to train it more. For instance we can feedforward the same 100 observations more than once. Despite the fact that these are the same observations, longer training allows NN to accumulate more knowledge. Keep in mind that due to the presence of a learning rate NN receives only part of the information that is available and useful to predict particular observation. Seeing the same observations several times leads to smaller loss of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's introduce one extra parameter called \"epochs\" and create a loop around the number of epochs. The rest of the code we see below is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance =  0.959\n"
     ]
    }
   ],
   "source": [
    "# The \"big loop\" with epochs:\n",
    "for e in range(epochs):\n",
    "    for i in data:\n",
    "        observation = i.split(',')\n",
    "        input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "        target = np.array(np.zeros(o_n) + 0.01, ndmin=2).T\n",
    "        target[int(observation[0])] = 0.99\n",
    "\n",
    "        h_input = np.dot(w_i_h, input)\n",
    "        h_output = sigmoid(h_input)\n",
    "        o_input = np.dot(w_h_o, h_output)\n",
    "        o_output = sigmoid(o_input)\n",
    "\n",
    "        o_errors = target - o_output\n",
    "        h_errors = np.dot(w_h_o.T, o_errors)\n",
    "        w_h_o += l_r * np.dot((o_errors * o_output * (1.0 - o_output)), np.transpose(h_output))\n",
    "        w_i_h += l_r * np.dot((h_errors * h_output * (1.0 - h_output)), np.transpose(input))\n",
    "\n",
    "        pass\n",
    "    pass\n",
    "\n",
    "\n",
    "# test\n",
    "score = []\n",
    "\n",
    "for i in data_test:\n",
    "    observation = i.split(',')\n",
    "    correct_label = int(observation[0])\n",
    "    input = np.array((np.asfarray(observation[1:])/255.0*0.99) + 0.01, ndmin=2).T\n",
    "\n",
    "    h_input = np.dot(w_i_h, input)\n",
    "    h_output = sigmoid(h_input)\n",
    "    o_input = np.dot(w_h_o, h_output)\n",
    "    o_output = sigmoid(o_input)\n",
    "\n",
    "    label = np.argmax(o_output)\n",
    "    if (label == correct_label):\n",
    "        score.append(1)\n",
    "    else:\n",
    "        score.append(0)\n",
    "        pass\n",
    "    \n",
    "    pass\n",
    "\n",
    "\n",
    "# calculate accuracy\n",
    "score_array = np.asarray(score)\n",
    "print (\"performance = \", score_array.sum() / score_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Training with other l_r**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smaller the learning rate the more capable the network to optimize the weights in a more accurate way. At the same time one should keep in mind that small l_r also means additional loss of information extracted from each particular observation. Hence, there should be many training observations available in order to make the trade-off between accuracy and usage of available data reasonable. Given that we have more epochs now, it is interesting to try a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_r = 0.1\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A more complicated structure**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may remember in the beginning we have assigned the number of nodes in the hidden layer based on some rule of thumb assumptions. Now we can test if the NN will perform better if we increase the number of hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h_n = 150\n",
    "\n",
    "# Determine the weights for a bigger matrices\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always possible to train neural networks where the number of neurons is larger. But, with a smaller number of neurons the neural network has much better generalization abilities. \n",
    "\n",
    "**Overfitting.** To many nodes is one of the reasons that leads to a problem when the neural network is over trained which would mean that it will fail to recognize patterns which were never used in the training.\n",
    "\n",
    "With a smaller number of neurons, it is more complicated to train the network to very small errors, but it may produce much better approximations for new patterns. The most common mistake made by many researchers is that in order to speed up the training process and to reduce the training errors, they use neural networks with a larger number of neurons than required. Such networks could perform poorly for new patterns not seen previously by the NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other training set**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other source of improvement is providing the NN with a relatively big dataset for training. Everything that was done before was implemented with just 100 observations. Let's see if our results improve if we increase our training dataset to 60 000 observations. As we have more data now we will reduce the number of epochs and keep having low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = open(\"data/mnist_train.csv\", 'r')\n",
    "data = raw_data.readlines()\n",
    "raw_data.close()\n",
    "\n",
    "# Settings\n",
    "epochs = 2\n",
    "l_r = 0.1\n",
    "h_n = 90\n",
    "w_i_h = np.random.normal(0.0, pow(h_n, -0.5), (h_n, i_n))\n",
    "w_h_o = np.random.normal(0.0, pow(o_n, -0.5), (o_n, h_n))\n",
    "\n",
    "# run the \"big loop\" with epochs again to get measure accuracy for new settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result we achieve with a big training set is already pretty impressive. In more than 90 % of cases our NN is able to solve the classification problem properly. And we should remember that it was implemented from scratch using only basic linear algebra packages. Let's see in the following section if we can do better or if we can simplify the process using specialized packages to build neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Conclusion:\n",
    "We have shown how to implement a neural network in python from scratch while explaining the intuition and mathematics behind each concept. We also compared the result of our work with other ready-to-use packages such as keras and we noticed that the accuracy of our basic neural network is reasonable taking into account the advanced optimizers used in the keras package.  The reader should now have a clear concept of how a neural network performs and what basic techniques can be applied to optimize it. Neural networks are nowadays very popular and there are a handful of literatures, some of which were discussed, on how to optimize them. They have a huge range of application in medicine, business, engineering and so forth. Although there was little room to discuss on a wide topic like neural network in a blog post, we tried our best to familiarize the readers with as many concepts as we can. Our discussion on the activation functions and optimizations were intentionally focused on the most common examples to provide reader with a better intuition. It should be noted that there are different types of neural nets and a variety of learning algorithms which were not covered in this blog. In contrast, the content in this blog should be perceived as a starting point for anyone who wants to advance their knowledge on the topic.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
